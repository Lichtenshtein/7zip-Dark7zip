From ea6457b4ba78a863837cd8afe63f091d55f5abfc Mon Sep 17 00:00:00 2001
From: Nobody <36154203+PopuriAO29@users.noreply.github.com>
Date: Sun, 14 Jan 2024 03:40:09 +0000
Subject: [PATCH]: Optimized_code_lzma_aes_benchmark_patch

---
diff -ruN 7zip-Dark7zip/Asm/x86/AesOpt.asm 7zip-Dark7zip2/Asm/x86/AesOpt.asm
--- 7zip-Dark7zip/Asm/x86/AesOpt.asm	2024-01-19 10:52:35.259212795 -0800
+++ 7zip-Dark7zip2/Asm/x86/AesOpt.asm	2024-01-19 13:13:01.399305243 -0800
@@ -5,7 +5,11 @@
 
 ifdef __ASMC__
   use_vaes_256 equ 1
+  use_vaes_512 equ 1
 else
+ifdef zmm0
+  use_vaes_512 equ 1
+endif
 ifdef ymm0
   use_vaes_256 equ 1
 endif
@@ -99,9 +103,12 @@
 state   equ  xmm0
 key     equ  xmm0
 key_ymm equ  ymm0
+key_zmm equ  zmm0
 key_ymm_n equ   0
+key_zmm_n equ   0
 
 ifdef x64
+        zways = 8
         ways = 11
 else
         ways = 4
@@ -111,6 +118,7 @@
 
 iv      equ     @CatStr(xmm, %(ways_start_reg + ways))
 iv_ymm  equ     @CatStr(ymm, %(ways_start_reg + ways))
+iv_zmm  equ     @CatStr(zmm, %(ways_start_reg + ways))
 
 
 WOP macro op, op2
@@ -255,10 +263,13 @@
 
 key0            equ  @CatStr(xmm, %(ways_start_reg + ways + 1))
 key0_ymm        equ  @CatStr(ymm, %(ways_start_reg + ways + 1))
+key0_zmm        equ  @CatStr(zmm, %(ways_start_reg + ways + 1))
 
 key_last        equ  @CatStr(xmm, %(ways_start_reg + ways + 2))
 key_last_ymm    equ  @CatStr(ymm, %(ways_start_reg + ways + 2))
+key_last_zmm    equ  @CatStr(zmm, %(ways_start_reg + ways + 2))
 key_last_ymm_n  equ                (ways_start_reg + ways + 2)
+key_last_zmm_n  equ                (ways_start_reg + ways + 2)
 
 NUM_CBC_REGS    equ  (ways_start_reg + ways + 3)
 
@@ -354,6 +365,14 @@
     endm
 endm
 
+AVX__WOP_Z_n macro op
+    i = 0
+    rept zways
+        op      (ways_start_reg + i)
+        i = i + 1
+    endm
+endm
+
 AVX__WOP macro op
     i = 0
     rept ways
@@ -362,18 +381,35 @@
     endm
 endm
 
+AVX__WOP_Z macro op
+    i = 0
+    rept zways
+        op      @CatStr(zmm, %(ways_start_reg + i))
+        i = i + 1
+    endm
+endm
+
 
 AVX__WOP_KEY macro op:req, offs:req
         vmovdqa  key_ymm, ymmword ptr [keys2 + offs]
         AVX__WOP_n op
 endm
 
+AVX__WOP_KEY_Z macro op:req, offs:req
+        vmovdqa32  key_zmm, zmmword ptr [keys2 + offs]
+        AVX__WOP_Z_n op
+endm
 
 AVX__CBC_START macro reg
         ; vpxor   reg, key_ymm, ymmword ptr [rD + 32 * i]
         vpxor   reg, key0_ymm, ymmword ptr [rD + 32 * i]
 endm
 
+AVX__CBC_START_Z macro reg
+        ; vpxor   reg, key_zmm, zmmword ptr [rD + 64 * i]
+        vpxord   reg, key0_zmm, zmmword ptr [rD + 64 * i]
+endm
+
 AVX__CBC_END macro reg
     if i eq 0
         vpxor   reg, reg, iv_ymm
@@ -382,11 +418,23 @@
     endif
 endm
 
+AVX__CBC_END_Z macro reg
+    if i eq 0
+        vpxord   reg, reg, iv_zmm
+    else
+        vpxord   reg, reg, zmmword ptr [rD + i * 64 - 16]
+    endif
+endm
+
 
 AVX__WRITE_TO_DATA macro reg
         vmovdqu ymmword ptr [rD + 32 * i], reg
 endm
 
+AVX__WRITE_TO_DATA_Z macro reg
+        vmovdqu32 zmmword ptr [rD + 64 * i], reg
+endm
+
 AVX__XOR_WITH_DATA macro reg
         vpxor   reg, reg, ymmword ptr [rD + 32 * i]
 endm
@@ -406,33 +454,66 @@
   db 0c0H + 8 * ((dest) and 7) + ((a2) and 7)
 endm
 
+MY_VAES_INSTR_2_Z macro cmd, dest, a1, a2
+  db 062H
+  db 2 + 050H + 020h * (1 - (a2) / 8) + 080h * (1 - (dest) / 8)
+  db 5 + 8 * ((not (a1)) and 15)
+  db 048H
+  db cmd
+  db 0c0H + 8 * ((dest) and 7) + ((a2) and 7)
+endm
+
 MY_VAES_INSTR macro cmd, dest, a
         MY_VAES_INSTR_2  cmd, dest, dest, a
 endm
 
+MY_VAES_INSTR_Z macro cmd, dest, a
+        MY_VAES_INSTR_2_Z  cmd, dest, dest, a
+endm
+
 MY_vaesenc macro dest, a
         MY_VAES_INSTR  0dcH, dest, a
 endm
+
 MY_vaesenclast macro dest, a
         MY_VAES_INSTR  0ddH, dest, a
 endm
+
 MY_vaesdec macro dest, a
         MY_VAES_INSTR  0deH, dest, a
 endm
+
+MY_vaesdec_Z macro dest, a
+        MY_VAES_INSTR_Z  0deH, dest, a
+endm
+
 MY_vaesdeclast macro dest, a
         MY_VAES_INSTR  0dfH, dest, a
 endm
 
+MY_vaesdeclast_Z macro dest, a
+        MY_VAES_INSTR_Z  0dfH, dest, a
+endm
+
 
 AVX__VAES_DEC macro reg
         MY_vaesdec reg, key_ymm_n
 endm
 
+AVX__VAES_DEC_Z macro reg
+        MY_vaesdec_Z reg, key_zmm_n
+endm
+
 AVX__VAES_DEC_LAST_key_last macro reg
         ; MY_vaesdeclast reg, key_ymm_n
         MY_vaesdeclast reg, key_last_ymm_n
 endm
 
+AVX__VAES_DEC_LAST_key_last_Z macro reg
+        ; MY_vaesdeclast reg, key_zmm_n
+        MY_vaesdeclast_Z reg, key_last_zmm_n
+endm
+
 AVX__VAES_ENC macro reg
         MY_vaesenc reg, key_ymm_n
 endm
@@ -445,9 +526,14 @@
         vinserti128  dest, dest, src, 1
 endm
 
+AVX__vinsert_values_TO_HIGH macro dest, src, index
+        vinserti32x4  dest, dest, src, index
+endm
+
 
 MY_PROC AesCbc_Decode_HW_256, 3
   ifdef use_vaes_256
+    AesCbc_Decode_HW_256_start::
         MY_PROLOG NUM_CBC_REGS
         
         cmp    rN, ways * 2
@@ -514,6 +600,75 @@
         jmp     AesCbc_Decode_HW_start
   endif
 MY_ENDP
+
+MY_PROC AesCbc_Decode_HW_512, 3
+  ifdef use_vaes_512
+        MY_PROLOG NUM_CBC_REGS
+
+        cmp    rN, zways * 4
+        jb     AesCbc_Decode_HW_start_2
+
+        vmovdqa iv, xmmword ptr [keys]
+        add     keys, 32
+
+        vbroadcasti32x4  key0_zmm, xmmword ptr [keys + 1 * ksize_r]
+        vbroadcasti32x4  key_last_zmm, xmmword ptr [keys]
+        sub     ksize_x, 16
+        mov     koffs_x, ksize_x
+        shl     ksize_x, 2
+
+        AVX_STACK_SUB = ((NUM_AES_KEYS_MAX + 1 - 2) * 64)
+        push    keys2
+        sub     r4, AVX_STACK_SUB
+        ; sub     r4, 64
+        ; sub     r4, ksize_r
+        ; lea     keys2, [r4 + 64]
+        mov     keys2, r4
+        and     keys2, -64
+    broad:
+        vbroadcasti32x4  key_zmm, xmmword ptr [keys + 1 * koffs_r]
+        vmovdqa32        zmmword ptr [keys2 + koffs_r * 4], key_zmm
+        sub     koffs_r, 16
+        ; jnc     broad
+        jnz     broad
+
+        sub     rN, zways * 4
+
+    align 16
+    avx_cbcdec_nextBlock2:
+        mov     koffs_x, ksize_x
+        AVX__WOP_Z    AVX__CBC_START_Z
+    @@:
+        AVX__WOP_KEY_Z    AVX__VAES_DEC_Z, 1 * koffs_r
+        sub     koffs_r, 64
+        jnz     @B
+        AVX__WOP_Z_n   AVX__VAES_DEC_LAST_key_last_Z
+
+        AVX__vinsert_values_TO_HIGH  iv_zmm, xmmword ptr [rD], 1
+        AVX__vinsert_values_TO_HIGH  iv_zmm, xmmword ptr [rD + 16], 2
+        AVX__vinsert_values_TO_HIGH  iv_zmm, xmmword ptr [rD + 32], 3
+        AVX__WOP_Z        AVX__CBC_END_Z
+
+        vmovdqa         iv, xmmword ptr [rD + zways * 64 - 16]
+        AVX__WOP_Z        AVX__WRITE_TO_DATA_Z
+
+        add     rD, zways * 64
+        sub     rN, zways * 4
+        jnc     avx_cbcdec_nextBlock2
+        add     rN, zways * 4
+
+        shr     ksize_x, 2
+
+        ; lea     r4, [r4 + 1 * ksize_r + 32]
+        add     r4, AVX_STACK_SUB
+        pop     keys2
+
+        vzeroupper
+        jmp     AesCbc_Decode_HW_start_3
+  else
+        jmp     AesCbc_Decode_HW_256_start
+  endif
+MY_ENDP
 MY_SEG_ENDP
 
 
@@ -739,4 +894,4 @@
 MY_ENDP
 MY_SEG_ENDP
 
-end
+end
\ No newline at end of file
diff -ruN 7zip-Dark7zip/Asm/x86/LzFindOpt.asm 7zip-Dark7zip2/Asm/x86/LzFindOpt.asm
--- 7zip-Dark7zip/Asm/x86/LzFindOpt.asm	2024-01-19 10:53:03.209213101 -0800
+++ 7zip-Dark7zip2/Asm/x86/LzFindOpt.asm	2024-01-19 10:54:25.619214006 -0800
@@ -137,7 +137,7 @@
 
 
 ; MY_ALIGN_64
-MY_PROC GetMatchesSpecN_2, 13
+MY_PROC GetMatchesSpecN_2_CPU, 13
 MY_PUSH_PRESERVED_ABI_REGS
         mov     r0, RSP
         lea     r3, [r0 - LOCAL_SIZE]
@@ -158,21 +158,21 @@
 endif
 
         COPY_VAR_64 limit_VAR, limit_PAR
-        
+
         mov     hash_lim, size_PAR
         mov     size_VAR, hash_lim
-        
+
         mov     cp_x, cycPos_PAR
         mov     hash, hash_PAR
 
         mov     cycSize, cycSize_PAR
         mov     cycSize_VAR, cycSize
-        
+
         ; we want cur in (rcx). So we change the cur and lenLimit variables
         sub     lenLimit, cur
         neg     lenLimit_x
         inc     lenLimit_x
-        
+
         mov     t0_x, maxLen_PAR
         sub     t0, lenLimit
         mov     maxLen_VAR, t0
@@ -197,23 +197,23 @@
         mov     diff_x, [hash]  ; delta
         add     hash, 4
         ; mov     cycPos_VAR, cp_x
-       
+
         inc     cur
         add     d, 4
         mov     m, pos
         sub     m, diff_x;      ; matchPos
-        
+
         ; CLzRef *ptr1 = son + ((size_t)(pos) << 1) - CYC_TO_POS_OFFSET * 2;
         lea     ptr1, [son + 8 * cp_r]
         ; mov     cycSize, cycSize_VAR
         cmp     pos, cycSize
         jb      directMode      ; if (pos < cycSize_VAR)
-        
+
         ; CYC MODE
 
         cmp     diff_x, cycSize
         jae     fill_empty      ; if (delta >= cycSize_VAR)
-        
+
         xor     t0_x, t0_x
         mov     cycPos_VAR, cp_x
         sub     cp_x, diff_x
@@ -222,13 +222,13 @@
         cmovb   t0_x, cycSize
         add     cp_x, t0_x      ; cp_x +=  (cycPos < delta ? cycSize : 0)
         jmp     prepare_for_tree_loop
-        
-        
+
+
 directMode:
         cmp     diff_x,  pos
         je      fill_empty      ; if (delta == pos)
         jae     fin_error       ; if (delta >= pos)
-        
+
         mov     cycPos_VAR, cp_x
         mov     cp_x, m
         
@@ -260,7 +260,7 @@
         lea     cp_r, [son + cp_r * 8]
         cmp     [cur + 1 * len], t0_L
         je      matched_1
-        
+
         jb      left_0
 
         mov     [ptr1], m
@@ -283,11 +283,11 @@
         mov     cycSize, cycSize_VAR
         dec     cutValueCur_VAR
         je      finish_tree
-        
+
         add     diff_x, pos     ; prev_match = pos + diff
         cmp     m, diff_x
         jae     fin_error       ; if (new_match >= prev_match)
-        
+
         mov     diff_x, pos
         sub     diff_x, m       ; delta = pos - new_match
         cmp     pos, cycSize
@@ -296,14 +296,14 @@
         mov     cp_x, m
         test    m, m
         jne     tree_loop       ; if (m != 0)
-        
+
 finish_tree:
         ; ptr0 = *ptr1 = kEmptyHashValue;
         mov     DWORD PTR [ptr0], 0
         mov     DWORD PTR [ptr1], 0
 
         inc     pos
-        
+
         ; _distances[-1] = (UInt32)(d - _distances);
         mov     t0, distances
         mov     t1, d
@@ -313,7 +313,7 @@
 
         cmp     d, limit_VAR
         jae     fin             ; if (d >= limit)
-   
+
         mov     cp_x, cycPos_VAR
         mov     hash, hash_VAR
         mov     hash_lim, size_VAR
@@ -321,7 +321,7 @@
         cmp     hash, hash_lim
         jne     main_loop       ; if (hash != size)
         jmp     fin
-        
+
 
 MY_ALIGN_32
 cyc_mode_2:
@@ -335,7 +335,7 @@
         add     cp_x, t0_x      ; cp_x += (cycPos < delta ? cycSize : 0)
         jmp     tree_loop
 
-        
+
 MY_ALIGN_32
 matched_1:
 
@@ -346,7 +346,7 @@
         cmp     [cur + 1 * len], t0_L
         jne     mismatch
 
-        
+
 MY_ALIGN_32
 match_loop:
         ;  while (++len != lenLimit)  (len[diff] != len[0]) ;
@@ -367,7 +367,7 @@
         mov     len1, len
 
         jmp     max_update
-        
+
 MY_ALIGN_32
 left_2:
         mov     [ptr0], m
@@ -380,7 +380,7 @@
 
         cmp     maxLen, len
         jae     next_node
-        
+
         mov     maxLen, len
         add     len, lenLimit
         mov     [d], len_x
@@ -388,11 +388,393 @@
         not     t0_x
         mov     [d + 4], t0_x
         add     d, 8
-       
+
         jmp     next_node
 
 
+
+MY_ALIGN_32
+lenLimit_reach:
+
+        mov     delta_r, cur
+        sub     delta_r, diff
+        lea     delta1_r, [delta_r - 1]
+
+        mov     t0_x, [cp_r]
+        mov     [ptr1], t0_x
+        mov     t0_x, [cp_r + 4]
+        mov     [ptr0], t0_x
+
+        mov     [d], lenLimit_x
+        mov     [d + 4], delta1_x
+        add     d, 8
+
+        ; _distances[-1] = (UInt32)(d - _distances);
+        mov     t0, distances
+        mov     t1, d
+        sub     t1, t0
+        shr     t1_x, 2
+        mov     [t0 - 4], t1_x
+
+        mov     hash, hash_VAR
+        mov     hash_lim, size_VAR
+
+        inc     pos
+        mov     cp_x, cycPos_VAR
+        inc     cp_x
+
+        mov     d_lim, limit_VAR
+        mov     cycSize, cycSize_VAR
+        ; if (hash == size || *hash != delta || lenLimit[diff] != lenLimit[0] || d >= limit)
+        ;    break;
+        cmp     hash, hash_lim
+        je      fin
+        cmp     d, d_lim
+        jae     fin
+        cmp     delta_x, [hash]
+        jne     main_loop
+        movzx   t0_x, BYTE PTR [diff]
+        cmp     [cur], t0_L
+        jne     main_loop
+
+        ; jmp     main_loop     ; bypass for debug
+
+        mov     cycPos_VAR, cp_x
+        shl     len, 3          ; cycSize * 8
+        sub     diff, cur       ; restore diff
+        xor     t0_x, t0_x
+        cmp     cp_x, delta_x   ; cmp (cycPos_VAR, delta)
+        lea     cp_r, [son + 8 * cp_r]  ; dest
+        lea     src, [cp_r + 8 * diff]
+        cmovb   t0, len         ; t0 =  (cycPos_VAR < delta ? cycSize * 8 : 0)
+        add     src, t0
+        add     len, son        ; len = son + cycSize * 8
+
+
+MY_ALIGN_32
+long_loop:
+        add     hash, 4
+
+        ; *(UInt64 *)(void *)ptr = ((const UInt64 *)(const void *)ptr)[diff];
+
+        mov     t0, [src]
+        add     src, 8
+        mov     [cp_r], t0
+        add     cp_r, 8
+        cmp     src, len
+        cmove   src, son       ; if end of (son) buffer is reached, we wrap to begin
+
+        mov     DWORD PTR [d], 2
+        mov     [d + 4], lenLimit_x
+        mov     [d + 8], delta1_x
+        add     d, 12
+
+        inc     cur
+
+        cmp     hash, hash_lim
+        je      long_footer
+        cmp     delta_x, [hash]
+        jne     long_footer
+        movzx   t0_x, BYTE PTR [diff + 1 * cur]
+        cmp     [cur], t0_L
+        jne     long_footer
+        cmp     d, d_lim
+        jb      long_loop
+
+long_footer:
+        sub     cp_r, son
+        shr     cp_r, 3
+        add     pos, cp_x
+        sub     pos, cycPos_VAR
+        mov     cycSize, cycSize_VAR
+
+        cmp     d, d_lim
+        jae     fin
+        cmp     hash, hash_lim
+        jne     main_loop
+        jmp     fin
+
+
+
+fin_error:
+        xor     d, d
+
+fin:
+        mov     RSP, Old_RSP
+        mov     t0, [r4 + posRes_OFFS]
+        mov     [t0], pos
+        mov     r0, d
+
+MY_POP_PRESERVED_ABI_REGS
+MY_ENDP
+
+; MY_ALIGN_64
+MY_PROC GetMatchesSpecN_2_512, 13
+MY_PUSH_PRESERVED_ABI_REGS
+        mov     r0, RSP
+        lea     r3, [r0 - LOCAL_SIZE]
+        and     r3, -64
+        mov     RSP, r3
+        mov     Old_RSP, r0
+
+if (IS_LINUX gt 0)
+        mov     d,            REG_ABI_PARAM_5       ; r13 = r9
+        mov     cutValue_VAR, REG_ABI_PARAM_4_x     ;     = r8
+        mov     son,          REG_ABI_PARAM_3       ;  r9 = r1
+        mov     r8,           REG_ABI_PARAM_2       ;  r8 = r2
+        mov     pos,          REG_ABI_PARAM_1_x     ;  r2 = x6
+        mov     r1,           REG_ABI_PARAM_0       ;  r1 = r7
+else
+        COPY_VAR_32 cutValue_VAR, cutValue_PAR
+        mov     d, d_PAR
+endif
+
+        COPY_VAR_64 limit_VAR, limit_PAR
+
+        mov     hash_lim, size_PAR
+        mov     size_VAR, hash_lim
+
+        mov     cp_x, cycPos_PAR
+        mov     hash, hash_PAR
+
+        mov     cycSize, cycSize_PAR
+        mov     cycSize_VAR, cycSize
+
+        ; we want cur in (rcx). So we change the cur and lenLimit variables
+        sub     lenLimit, cur
+        neg     lenLimit_x
+        inc     lenLimit_x
+
+        mov     t0_x, maxLen_PAR
+        sub     t0, lenLimit
+        mov     maxLen_VAR, t0
+
+        jmp     main_loop
+
+MY_ALIGN_64
+fill_empty:
+        ; ptr0 = *ptr1 = kEmptyHashValue;
+        mov     QWORD PTR [ptr1], 0
+        inc     pos
+        inc     cp_x
+        mov     DWORD PTR [d - 4], 0
+        cmp     d, limit_VAR
+        jae     fin
+        cmp     hash, hash_lim
+        je      fin
+
+; MY_ALIGN_64
+main_loop:
+        ; UInt32 delta = *hash++;
+        mov     diff_x, [hash]  ; delta
+        add     hash, 4
+        ; mov     cycPos_VAR, cp_x
+
+        inc     cur
+        add     d, 4
+        mov     m, pos
+        sub     m, diff_x;      ; matchPos
+
+        ; CLzRef *ptr1 = son + ((size_t)(pos) << 1) - CYC_TO_POS_OFFSET * 2;
+        lea     ptr1, [son + 8 * cp_r]
+        ; mov     cycSize, cycSize_VAR
+        cmp     pos, cycSize
+        jb      directMode      ; if (pos < cycSize_VAR)
+
+        ; CYC MODE
+
+        cmp     diff_x, cycSize
+        jae     fill_empty      ; if (delta >= cycSize_VAR)
+
+        xor     t0_x, t0_x
+        mov     cycPos_VAR, cp_x
+        sub     cp_x, diff_x
+        ; jae     prepare_for_tree_loop
+        ; add     cp_x, cycSize
+        cmovb   t0_x, cycSize
+        add     cp_x, t0_x      ; cp_x +=  (cycPos < delta ? cycSize : 0)
+        jmp     prepare_for_tree_loop
+
+
+directMode:
+        cmp     diff_x,  pos
+        je      fill_empty      ; if (delta == pos)
+        jae     fin_error       ; if (delta >= pos)
         
+        mov     cycPos_VAR, cp_x
+        mov     cp_x, m
+
+prepare_for_tree_loop:
+        mov     len0, lenLimit
+        mov     hash_VAR, hash
+        ; CLzRef *ptr0 = son + ((size_t)(pos) << 1) - CYC_TO_POS_OFFSET * 2 + 1;
+        lea     ptr0, [ptr1 + 4]
+        ; UInt32 *_distances = ++d;
+        mov     distances, d
+
+        neg     len0
+        mov     len1, len0
+
+        mov     t0_x, cutValue_VAR
+        mov     maxLen, maxLen_VAR
+        mov     cutValueCur_VAR, t0_x
+
+MY_ALIGN_32
+tree_loop:
+        neg     diff
+        mov     len, len0
+        cmp     len1, len0
+        cmovb   len, len1       ; len = (len1 < len0 ? len1 : len0);
+        add     diff, cur
+
+        mov     t0_x, [son + cp_r * 8]  ; prefetch
+        movzx   t0_x, BYTE PTR [diff + 1 * len]
+        lea     cp_r, [son + cp_r * 8]
+        cmp     [cur + 1 * len], t0_L
+        je      matched_1
+
+        jb      left_0
+
+        mov     [ptr1], m
+        mov        m, [cp_r + 4]
+        lea     ptr1, [cp_r + 4]
+        sub     diff, cur ; FIX32
+        jmp     next_node
+
+MY_ALIGN_32
+left_0:
+        mov     [ptr0], m
+        mov        m, [cp_r]
+        mov     ptr0, cp_r
+        sub     diff, cur ; FIX32
+        ; jmp     next_node
+
+; ------------ NEXT NODE ------------
+; MY_ALIGN_32
+next_node:
+        mov     cycSize, cycSize_VAR
+        dec     cutValueCur_VAR
+        je      finish_tree
+
+        add     diff_x, pos     ; prev_match = pos + diff
+        cmp     m, diff_x
+        jae     fin_error       ; if (new_match >= prev_match)
+
+        mov     diff_x, pos
+        sub     diff_x, m       ; delta = pos - new_match
+        cmp     pos, cycSize
+        jae     cyc_mode_2      ; if (pos >= cycSize)
+
+        mov     cp_x, m
+        test    m, m
+        jne     tree_loop       ; if (m != 0)
+
+finish_tree:
+        ; ptr0 = *ptr1 = kEmptyHashValue;
+        mov     DWORD PTR [ptr0], 0
+        mov     DWORD PTR [ptr1], 0
+
+        inc     pos
+
+        ; _distances[-1] = (UInt32)(d - _distances);
+        mov     t0, distances
+        mov     t1, d
+        sub     t1, t0
+        shr     t1_x, 2
+        mov     [t0 - 4], t1_x
+
+        cmp     d, limit_VAR
+        jae     fin             ; if (d >= limit)
+
+        mov     cp_x, cycPos_VAR
+        mov     hash, hash_VAR
+        mov     hash_lim, size_VAR
+        inc     cp_x
+        cmp     hash, hash_lim
+        jne     main_loop       ; if (hash != size)
+        jmp     fin
+
+
+MY_ALIGN_32
+cyc_mode_2:
+        cmp     diff_x, cycSize
+        jae     finish_tree     ; if (delta >= cycSize)
+
+        mov     cp_x, cycPos_VAR
+        xor     t0_x, t0_x
+        sub     cp_x, diff_x    ; cp_x = cycPos - delta
+        cmovb   t0_x, cycSize
+        add     cp_x, t0_x      ; cp_x += (cycPos < delta ? cycSize : 0)
+        jmp     tree_loop
+
+
+MY_ALIGN_32
+matched_1:
+
+        inc     len
+        ; cmp     len_x, lenLimit_x
+        je      lenLimit_reach
+        movzx   t0_x, BYTE PTR [diff + 1 * len]
+        cmp     [cur + 1 * len], t0_L
+        jne     mismatch
+
+MY_ALIGN_32
+match_loop:
+        ;  while (len != lenLimit)  (matchLen != 16) ; len = len + matchLen
+
+        vmovdqu    xmm1, XMMWORD PTR [diff + 1 * len]
+        vmovdqu    xmm0, XMMWORD PTR [cur + 1 * len]
+        ;sse variant
+        ;pcmpeqb    xmm1, xmm0
+        ;pmovmskb   t0_x, xmm1
+        ;not        t0_x
+        ;avx512 variant
+        vpcmpb     k1, xmm0, xmm1, 4
+        kmovw      t0_x, k1
+        bts        t0_x, 16
+        bsf        t0_x, t0_x
+
+        add        len, t0
+        jge        lenLimit_check
+        cmp        t0_x, 16
+        jne        mismatch
+        jmp        match_loop
+
+lenLimit_check:
+        mov       len, 0
+        jmp       lenLimit_reach
+
+mismatch:
+        movzx   t0_x, BYTE PTR [diff + 1 * len]
+        cmp     BYTE PTR [cur + 1 * len], t0_L
+
+        jb      left_2
+        mov     [ptr1], m
+        mov        m, [cp_r + 4]
+        lea     ptr1, [cp_r + 4]
+        mov     len1, len
+        jmp     max_update
+
+MY_ALIGN_32
+left_2:
+        mov     [ptr0], m
+        mov        m, [cp_r]
+        mov     ptr0, cp_r
+        mov     len0, len
+
+max_update:
+        sub     diff, cur       ; restore diff
+        cmp     maxLen, len
+        jae     next_node
+        mov     maxLen, len
+        add     len, lenLimit
+        mov     [d], len_x
+        mov     t0_x, diff_x
+        not     t0_x
+        mov     [d + 4], t0_x
+        add     d, 8
+        jmp     next_node
+
 MY_ALIGN_32
 lenLimit_reach:
 
diff -ruN 7zip-Dark7zip/C/Aes.c 7zip-Dark7zip2/C/Aes.c
--- 7zip-Dark7zip/C/Aes.c	2024-01-19 10:53:03.209213101 -0800
+++ 7zip-Dark7zip2/C/Aes.c	2024-01-19 10:53:47.639213589 -0800
@@ -145,6 +145,16 @@
       flags |= k_Aes_SupportedFunctions_HW_256;
       #endif
     }
+    if(CPU_IsSupported_VAES_AVX512())
+    {
+      PRF(printf("\n===vaes avx512\n"));
+      d = AesCbc_Decode_HW_512;
+      #ifndef Z7_SFX
+      c = AesCtr_Code_HW_256;
+      flags |= k_Aes_SupportedFunctions_HW_512;
+      #endif
+
+    }
     #endif
   }
   #endif
diff -ruN 7zip-Dark7zip/C/Aes.h 7zip-Dark7zip2/C/Aes.h
--- 7zip-Dark7zip/C/Aes.h	2024-01-19 10:53:03.209213101 -0800
+++ 7zip-Dark7zip2/C/Aes.h	2024-01-19 10:53:47.639213589 -0800
@@ -37,6 +37,7 @@
 extern AES_CODE_FUNC g_AesCtr_Code;
 #define k_Aes_SupportedFunctions_HW     (1 << 2)
 #define k_Aes_SupportedFunctions_HW_256 (1 << 3)
+#define k_Aes_SupportedFunctions_HW_512 (1 << 4)
 extern UInt32 g_Aes_SupportedFunctions_Flags;
 #endif
 
@@ -53,6 +54,7 @@
 Z7_DECLARE_AES_CODE_FUNC (AesCtr_Code_HW)
 
 Z7_DECLARE_AES_CODE_FUNC (AesCbc_Decode_HW_256)
+Z7_DECLARE_AES_CODE_FUNC (AesCbc_Decode_HW_512)
 Z7_DECLARE_AES_CODE_FUNC (AesCtr_Code_HW_256)
 
 EXTERN_C_END
diff -ruN 7zip-Dark7zip/C/AesOpt.c 7zip-Dark7zip2/C/AesOpt.c
--- 7zip-Dark7zip/C/AesOpt.c	2024-01-19 10:53:03.209213101 -0800
+++ 7zip-Dark7zip2/C/AesOpt.c	2024-01-19 10:53:47.639213589 -0800
@@ -24,8 +24,8 @@
       #if defined(__clang__) && (__clang_major__ >= 8) \
           || defined(__GNUC__) && (__GNUC__ >= 8)
         #define USE_INTEL_VAES
-        #if !defined(__AES__) || !defined(__VAES__) || !defined(__AVX__) || !defined(__AVX2__)
-          #define ATTRIB_VAES __attribute__((__target__("aes,vaes,avx,avx2")))
+        #if !defined(__AES__) || !defined(__VAES__) || !defined(__AVX__) || !defined(__AVX2__) || !defined(__AVX512F__)
+          #define ATTRIB_VAES __attribute__((__target__("aes,vaes,avx,avx2,avx512f")))
         #endif
       #endif
   #elif defined(_MSC_VER)
@@ -70,6 +70,7 @@
 
 #define MM_XOR( dest, src)    MM_OP(_mm_xor_si128,    dest, src)
 #define AVX_XOR(dest, src)    MM_OP(_mm256_xor_si256, dest, src)
+#define AVX512_XOR(dest, src)    MM_OP(_mm512_xor_si512, dest, src)
 
 
 AES_FUNC_START2 (AesCbc_Encode_HW)
@@ -112,14 +113,7 @@
 #define WOP_7(op)   WOP_6 (op)  op (m6, 6)
 #define WOP_8(op)   WOP_7 (op)  op (m7, 7)
 #endif
-/*
-#define WOP_9(op)   WOP_8 (op)  op (m8, 8);
-#define WOP_10(op)  WOP_9 (op)  op (m9, 9);
-#define WOP_11(op)  WOP_10(op)  op (m10, 10);
-#define WOP_12(op)  WOP_11(op)  op (m11, 11);
-#define WOP_13(op)  WOP_12(op)  op (m12, 12);
-#define WOP_14(op)  WOP_13(op)  op (m13, 13);
-*/
+
 
 #ifdef MY_CPU_AMD64
   #define NUM_WAYS      8
@@ -129,8 +123,25 @@
   #define WOP_M1    WOP_4
 #endif
 
-#define WOP(op)  op (m0, 0)  WOP_M1(op)
+#ifdef USE_INTEL_VAES
+#define WOP_5(op)   WOP_4 (op)  op (m4, 4)
+#define WOP_6(op)   WOP_5 (op)  op (m5, 5)
+#define WOP_7(op)   WOP_6 (op)  op (m6, 6)
+#define WOP_8(op)   WOP_7 (op)  op (m7, 7)
+#define WOP_9(op)   WOP_8 (op)  op (m8, 8)
+#define WOP_10(op)  WOP_9 (op)  op (m9, 9)
+#define WOP_11(op)  WOP_10(op)  op (m10, 10)
+#define WOP_12(op)  WOP_11(op)  op (m11, 11)
+#define WOP_13(op)  WOP_12(op)  op (m12, 12)
+#define WOP_14(op)  WOP_13(op)  op (m13, 13)
+#define WOP_15(op)  WOP_14(op)  op (m14, 14)
+#define WOP_16(op)  WOP_15(op)  op (m15, 15)
+#define WOP_MAVX1    WOP_8
+#define NUM_WAYSAVX    4
+#endif
 
+#define WOP(op)  op (m0, 0)  WOP_M1(op)
+#define WAVXOP(op)  op (m0, 0)  WOP_MAVX1(op)
 
 #define DECLARE_VAR(reg, ii)  __m128i reg;
 #define LOAD_data(  reg, ii)  reg = data[ii];
@@ -144,6 +155,11 @@
 #define AVX_STORE_data( reg, ii)  ((__m256i *)(void *)data)[ii] = reg;
 #define AVX_XOR_data_M1(reg, ii)  AVX_XOR (reg, (((const __m256i *)(const void *)(data - 1))[ii]))
 
+#define AVX512_DECLARE_VAR(reg, ii)  __m512i reg;
+#define AVX512_LOAD_data(  reg, ii)  reg = ((const __m512i *)(const void *)data)[ii];
+#define AVX512_STORE_data( reg, ii)  ((__m512i *)(void *)data)[ii] = reg;
+#define AVX512_XOR_data_M1(reg, ii)  AVX512_XOR (reg, (((const __m512i *)(const void *)(data - 1))[ii]))
+
 #define MM_OP_key(op, reg)  MM_OP(op, reg, key);
 
 #define AES_DEC(      reg, ii)   MM_OP_key (_mm_aesdec_si128,     reg)
@@ -159,6 +175,12 @@
 #define AVX_AES_ENC_LAST( reg, ii)   MM_OP_key (_mm256_aesenclast_epi128, reg)
 #define AVX_AES_XOR(      reg, ii)   MM_OP_key (_mm256_xor_si256,         reg)
 
+#define AVX512_AES_DEC(      reg, ii)   MM_OP_key (_mm512_aesdec_epi128,     reg)
+#define AVX512_AES_DEC_LAST( reg, ii)   MM_OP_key (_mm512_aesdeclast_epi128, reg)
+#define AVX512_AES_ENC(      reg, ii)   MM_OP_key (_mm512_aesenc_epi128,     reg)
+#define AVX512_AES_ENC_LAST( reg, ii)   MM_OP_key (_mm512_aesenclast_epi128, reg)
+#define AVX512_AES_XOR(      reg, ii)   MM_OP_key (_mm512_xor_si512,         reg)
+
 #define CTR_START(reg, ii)  MM_OP (_mm_add_epi64, ctr, one)  reg = ctr;
 #define CTR_END(  reg, ii)  MM_XOR (data[ii], reg)
 
@@ -173,6 +195,9 @@
     const __m256i key = w[n]; \
     WOP(op); }
 
+#define AVX512_WOP_KEY(op, n) { \
+    const __m512i key = w[n]; \
+    WAVXOP(op); }
 
 #define WIDE_LOOP_START  \
     dataEnd = data + numBlocks;  \
@@ -211,6 +236,26 @@
     _mm256_zeroupper();  \
     }  \
 
+
+#define WIDE_LOOP_START_AVX512(OP)  \
+    dataEnd = data + numBlocks;  \
+    if (numBlocks >= NUM_WAYSAVX * 8)  \
+    { __m512i keys[NUM_AES_KEYS_MAX]; \
+    UInt32 ii; \
+    OP \
+    for (ii = 0; ii < numRounds; ii++) \
+      keys[ii] = _mm512_broadcast_i32x4(p[ii]); \
+    dataEnd -= NUM_WAYSAVX * 8; do {  \
+
+
+#define WIDE_LOOP_END_AVX512(OP)  \
+    data += NUM_WAYSAVX * 8;  \
+    } while (data <= dataEnd);  \
+    dataEnd += NUM_WAYSAVX * 8;  \
+    OP  \
+    _mm256_zeroupper();  \
+    }  \
+
 /* MSVC for x86: If we don't call _mm256_zeroupper(), and -arch:IA32 is not specified,
    MSVC still can insert vzeroupper instruction. */
 
@@ -440,6 +485,67 @@
   p[-2] = iv;
 }
 
+//Implements AVX512+VAES based decoding operation
+//Uses 16 vectors of 512 bits to process 1024 bytes per single iteration.
+VAES_FUNC_START2 (AesCbc_Decode_HW_512)
+{
+  __m128i *p = (__m128i *)(void *)ivAes;
+  __m128i *data = (__m128i *)(void *)data8;
+  __m128i iv = *p;
+  __m512i ivdata = _mm512_setzero_si512();
+  const __m128i *dataEnd;
+  UInt32 numRounds = *(const UInt32 *)(p + 1) * 2 + 1;
+  p += 2;
+
+  WIDE_LOOP_START_AVX512(;)
+  {
+    const __m512i *w = keys + numRounds - 2;
+
+    WAVXOP (AVX512_DECLARE_VAR)
+    WAVXOP (AVX512_LOAD_data)
+    AVX512_WOP_KEY (AVX512_AES_XOR, 1)
+
+    do
+    {
+      AVX512_WOP_KEY (AVX512_AES_DEC, 0)
+      w--;
+    }
+    while (w != keys);
+    AVX512_WOP_KEY (AVX512_AES_DEC_LAST, 0)
+
+    //Creates first vector with iv and data vector for XOR with m0
+    ivdata = _mm512_inserti32x4(ivdata, iv, 0);
+    ivdata = _mm512_inserti32x4(ivdata, data[0], 1);
+    ivdata = _mm512_inserti32x4(ivdata, data[1], 2);
+    ivdata = _mm512_inserti32x4(ivdata, data[2], 3);
+    AVX512_XOR (m0, ivdata)
+    WOP_MAVX1 (AVX512_XOR_data_M1)
+    iv = data[NUM_WAYSAVX * 8 - 1];
+    WAVXOP (AVX512_STORE_data)
+  }
+  WIDE_LOOP_END_AVX512(;)
+
+  SINGLE_LOOP
+  {
+    const __m128i *w = p + *(const UInt32 *)(p + 1 - 2) * 2 + 1 - 3;
+    __m128i m = _mm_xor_si128 (w[2], *data);
+    do
+    {
+      MM_OP_m (_mm_aesdec_si128, w[1])
+      MM_OP_m (_mm_aesdec_si128, w[0])
+      w -= 2;
+    }
+    while (w != p);
+    MM_OP_m (_mm_aesdec_si128,     w[1])
+    MM_OP_m (_mm_aesdeclast_si128, w[0])
+
+    MM_XOR (m, iv)
+    iv = *data;
+    *data = m;
+  }
+
+  p[-2] = iv;
+}
 
 /*
 SSE2: _mm_cvtsi32_si128 : movd
@@ -831,7 +937,9 @@
 #endif // MY_CPU_ARM_OR_ARM64
 
 #undef NUM_WAYS
+#undef NUM_WAYSAVX
 #undef WOP_M1
+#undef WOP_MAVX1
 #undef WOP
 #undef DECLARE_VAR
 #undef LOAD_data
diff -ruN 7zip-Dark7zip/C/CpuArch.c 7zip-Dark7zip2/C/CpuArch.c
--- 7zip-Dark7zip/C/CpuArch.c	2024-01-19 10:53:03.209213101 -0800
+++ 7zip-Dark7zip2/C/CpuArch.c	2024-01-19 10:53:47.639213589 -0800
@@ -646,6 +646,19 @@
   // since Win7SP1: we can use GetEnabledXStateFeatures();
 }
 
+BoolInt CPU_IsSupported_AVX512(void)
+{
+  if (!CPU_IsSupported_AVX2())
+    return False;
+  if (z7_x86_cpuid_GetMaxFunc() < 7)
+    return False;
+  {
+    UInt32 d[4];
+    z7_x86_cpuid(d, 7);
+    return 1
+      & (d[1] >> 16); // avx512
+  }
+}
 
 BoolInt CPU_IsSupported_AVX2(void)
 {
@@ -678,6 +691,21 @@
       & (d[2] >> 9); // vaes // VEX-256/EVEX
   }
 }
+
+BoolInt CPU_IsSupported_VAES_AVX512(void)
+{
+  if (!CPU_IsSupported_AVX2())
+    return False;
+  if (z7_x86_cpuid_GetMaxFunc() < 7)
+    return False;
+  {
+    UInt32 d[4];
+    z7_x86_cpuid(d, 7);
+    return 1
+      & (d[1] >> 16) // avx512
+      & (d[2] >> 9); // vaes // VEX-256/EVEX
+  }
+}
 
 BoolInt CPU_IsSupported_PageGB(void)
 {
diff -ruN 7zip-Dark7zip/C/CpuArch.h 7zip-Dark7zip2/C/CpuArch.h
--- 7zip-Dark7zip/C/CpuArch.h	2024-01-19 10:53:03.209213101 -0800
+++ 7zip-Dark7zip2/C/CpuArch.h	2024-01-19 10:53:47.639213589 -0800
@@ -486,7 +486,9 @@
 BoolInt CPU_IsSupported_AES(void);
 BoolInt CPU_IsSupported_AVX(void);
 BoolInt CPU_IsSupported_AVX2(void);
+BoolInt CPU_IsSupported_AVX512(void);
 BoolInt CPU_IsSupported_VAES_AVX2(void);
+BoolInt CPU_IsSupported_VAES_AVX512(void);
 BoolInt CPU_IsSupported_CMOV(void);
 BoolInt CPU_IsSupported_SSE(void);
 BoolInt CPU_IsSupported_SSE2(void);
diff -ruN 7zip-Dark7zip/C/LzFind.c 7zip-Dark7zip2/C/LzFind.c
--- 7zip-Dark7zip/C/LzFind.c	2024-01-19 10:53:03.209213101 -0800
+++ 7zip-Dark7zip2/C/LzFind.c	2024-01-22 06:37:46.421888903 -0800
@@ -593,17 +593,23 @@
   #if defined(__clang__) && (__clang_major__ >= 4) \
     || defined(Z7_GCC_VERSION) && (Z7_GCC_VERSION >= 40701)
     // || defined(__INTEL_COMPILER) && (__INTEL_COMPILER >= 1900)
-
-      #define USE_LZFIND_SATUR_SUB_128
-      #define USE_LZFIND_SATUR_SUB_256
+      #define USE_LZFIND_FUNCTIONS_128
+      #define USE_LZFIND_FUNCTIONS_256
       #define LZFIND_ATTRIB_SSE41 __attribute__((__target__("sse4.1")))
       #define LZFIND_ATTRIB_AVX2  __attribute__((__target__("avx2")))
+      #if defined(__clang__) && (__clang_major__ >= 4) \
+        || defined(Z7_GCC_VERSION) && (Z7_GCC_VERSION >= 50000)
+          #define USE_LZFIND_FUNCTIONS_512
+      #endif
   #elif defined(_MSC_VER)
     #if (_MSC_VER >= 1600)
-      #define USE_LZFIND_SATUR_SUB_128
+      #define USE_LZFIND_FUNCTIONS_128
     #endif
     #if (_MSC_VER >= 1900)
-      #define USE_LZFIND_SATUR_SUB_256
+      #define USE_LZFIND_FUNCTIONS_256
+    #endif
+    #if (_MSC_VER >= 1920)
+      #define USE_LZFIND_FUNCTIONS_512
     #endif
   #endif
 
@@ -612,7 +618,7 @@
 
   #if defined(__clang__) && (__clang_major__ >= 8) \
     || defined(__GNUC__) && (__GNUC__ >= 8)
-      #define USE_LZFIND_SATUR_SUB_128
+      #define USE_LZFIND_FUNCTIONS_128
     #ifdef MY_CPU_ARM64
       // #define LZFIND_ATTRIB_SSE41 __attribute__((__target__("")))
     #else
@@ -621,7 +627,7 @@
 
   #elif defined(_MSC_VER)
     #if (_MSC_VER >= 1910)
-      #define USE_LZFIND_SATUR_SUB_128
+      #define USE_LZFIND_FUNCTIONS_128
     #endif
   #endif
 
@@ -634,7 +640,7 @@
 #endif
 
 
-#ifdef USE_LZFIND_SATUR_SUB_128
+#ifdef USE_LZFIND_FUNCTIONS_128
 
 // #define Z7_SHOW_HW_STATUS
 
@@ -699,7 +705,7 @@
 
 
 
-#ifdef USE_LZFIND_SATUR_SUB_256
+#ifdef USE_LZFIND_FUNCTIONS_256
 
 #include <immintrin.h> // avx
 /*
@@ -741,7 +747,39 @@
   }
   while (items != lim);
 }
-#endif // USE_LZFIND_SATUR_SUB_256
+#endif // USE_LZFIND_FUNCTIONS_256
+
+#ifdef USE_LZFIND_FUNCTIONS_512
+
+#ifdef _MSC_VER
+#include <intrin.h>
+#else
+#include <immintrin.h>
+#endif
+
+#ifdef __GNUC__
+#define MatchScanForward(x, m) *(x) = (unsigned long)__builtin_ctz(m)
+#elif _MSC_VER
+/* BitScanForward is Visual Studio specific. */
+#define MatchScanForward(x,m) _BitScanForward(x,m)
+#endif
+
+#if (defined(__AVX512VL__) && defined(__AVX512BW__))
+  #define mm_cmpare_epi8_mask(x,y) ((_mm_cmpneq_epi8_mask( x, y )) | 0x10000)
+#else
+  #define mm_cmpare_epi8_mask(x,y) (~(_mm_movemask_epi8( _mm_cmpeq_epi8( x, y ))) | 0x10000)
+#endif
+
+#if (defined(__GNUC__) && (__GNUC__ <= 10) && !defined(__clang__))
+  #define mm_loadu_epi8(x) _mm_loadu_si128((__m128i*)(x))
+#else
+  #if (defined(__AVX512VL__) && defined(__AVX512BW__))
+    #define mm_loadu_epi8(x) _mm_loadu_epi8(x)
+  #else
+    #define mm_loadu_epi8(x) _mm_loadu_si128((__m128i*)(x))
+  #endif
+#endif
+#endif
 
 #ifndef FORCE_LZFIND_SATUR_SUB_128
 typedef void (Z7_FASTCALL *LZFIND_SATUR_SUB_CODE_FUNC)(
@@ -749,8 +787,15 @@
 static LZFIND_SATUR_SUB_CODE_FUNC g_LzFind_SaturSub;
 #endif // FORCE_LZFIND_SATUR_SUB_128
 
-#endif // USE_LZFIND_SATUR_SUB_128
+#endif // USE_LZFIND_FUNCTIONS_128
 
+typedef UInt32 * (*MatchFinder_GetMatches_Function) (CMatchFinder *p, UInt32 *distances);
+
+static MatchFinder_GetMatches_Function Bt3_MatchFinder_GetMatches;
+static MatchFinder_GetMatches_Function Bt4_MatchFinder_GetMatches;
+static MatchFinder_GetMatches_Function Bt5_MatchFinder_GetMatches;
+static MatchFinder_GetMatches_Function Hc4_MatchFinder_GetMatches;
+static MatchFinder_GetMatches_Function Hc5_MatchFinder_GetMatches;
 
 // kEmptyHashValue must be zero
 // #define SASUB_32(i)  { UInt32 v = items[i];  UInt32 m = v - subValue;  if (v < subValue) m = kEmptyHashValue;  items[i] = m; }
@@ -800,7 +845,7 @@
     numItems &= k_Align_Mask;
     if (items != lim)
     {
-      #if defined(USE_LZFIND_SATUR_SUB_128) && !defined(FORCE_LZFIND_SATUR_SUB_128)
+      #if defined(USE_LZFIND_FUNCTIONS_128) && !defined(FORCE_LZFIND_SATUR_SUB_128)
         if (g_LzFind_SaturSub)
           g_LzFind_SaturSub(subValue, items, lim);
         else
@@ -952,7 +997,7 @@
 
 
 Z7_FORCE_INLINE
-UInt32 * GetMatchesSpec1(UInt32 lenLimit, UInt32 curMatch, UInt32 pos, const Byte *cur, CLzRef *son,
+static UInt32 * GetMatchesSpec1_CPU(UInt32 lenLimit, UInt32 curMatch, UInt32 pos, const Byte *cur, CLzRef *son,
     size_t _cyclicBufferPos, UInt32 _cyclicBufferSize, UInt32 cutValue,
     UInt32 *d, UInt32 maxLen)
 {
@@ -1021,6 +1066,88 @@
   return d;
 }
 
+#ifdef USE_LZFIND_FUNCTIONS_512
+Z7_FORCE_INLINE
+static UInt32 * GetMatchesSpec1_512(UInt32 lenLimit, UInt32 curMatch, UInt32 pos, const Byte *cur, CLzRef *son,
+    size_t _cyclicBufferPos, UInt32 _cyclicBufferSize, UInt32 cutValue,
+    UInt32 *d, UInt32 maxLen)
+{
+  CLzRef *ptr0 = son + ((size_t)_cyclicBufferPos << 1) + 1;
+  CLzRef *ptr1 = son + ((size_t)_cyclicBufferPos << 1);
+  unsigned len0 = 0, len1 = 0;
+
+  UInt32 cmCheck;
+
+  // if (curMatch >= pos) { *ptr0 = *ptr1 = kEmptyHashValue; return NULL; }
+
+  cmCheck = (UInt32)(pos - _cyclicBufferSize);
+  if ((UInt32)pos <= _cyclicBufferSize)
+    cmCheck = 0;
+
+  if (cmCheck < curMatch)
+  do
+  {
+    const UInt32 delta = pos - curMatch;
+    {
+      CLzRef *pair = son + ((size_t)(_cyclicBufferPos - delta + ((delta > _cyclicBufferPos) ? _cyclicBufferSize : 0)) << 1);
+      const Byte *pb = cur - delta;
+      unsigned len = (len0 < len1 ? len0 : len1);
+      const UInt32 pair0 = pair[0];
+      unsigned long matchValue = 16;
+      if (pb[len] == cur[len])
+      {
+        if (++len != lenLimit && pb[len] == cur[len]){
+          while(len != lenLimit)
+          {
+              __m128i pbVector = mm_loadu_epi8(pb + len);
+              __m128i curVector = mm_loadu_epi8(cur + len);
+              MatchScanForward(&matchValue, mm_cmpare_epi8_mask(pbVector, curVector));
+              len = len + (unsigned)matchValue;
+              if (matchValue != 16) break;
+          }
+          if(len >= lenLimit) {
+            len = lenLimit;
+          }
+        }
+        if (maxLen < len)
+        {
+          maxLen = (UInt32)len;
+          *d++ = (UInt32)len;
+          *d++ = delta - 1;
+          if (len == lenLimit)
+          {
+            *ptr1 = pair0;
+            *ptr0 = pair[1];
+            return d;
+          }
+        }
+      }
+      if (pb[len] < cur[len])
+      {
+        *ptr1 = curMatch;
+        // const UInt32 curMatch2 = pair[1];
+        // if (curMatch2 >= curMatch) { *ptr0 = *ptr1 = kEmptyHashValue;  return NULL; }
+        // curMatch = curMatch2;
+        curMatch = pair[1];
+        ptr1 = pair + 1;
+        len1 = len;
+      }
+      else
+      {
+        *ptr0 = curMatch;
+        curMatch = pair[0];
+        ptr0 = pair;
+        len0 = len;
+      }
+    }
+  }
+  while(--cutValue && cmCheck < curMatch);
+
+  *ptr0 = *ptr1 = kEmptyHashValue;
+  return d;
+}
+#endif
+
 
 static void SkipMatchesSpec(UInt32 lenLimit, UInt32 curMatch, UInt32 pos, const Byte *cur, CLzRef *son,
     size_t _cyclicBufferPos, UInt32 _cyclicBufferSize, UInt32 cutValue)
@@ -1118,28 +1245,55 @@
   distances = func(MF_PARAMS(p), \
   distances, (UInt32)_maxLen_); MOVE_POS_RET
 
-#define GET_MATCHES_FOOTER_BT(_maxLen_) \
-  GET_MATCHES_FOOTER_BASE(_maxLen_, GetMatchesSpec1)
+#define GET_MATCHES_FOOTER_BT_CPU(_maxLen_) \
+  GET_MATCHES_FOOTER_BASE(_maxLen_, GetMatchesSpec1_CPU)
+
+#ifdef USE_LZFIND_FUNCTIONS_512
+#define GET_MATCHES_FOOTER_BT_512(_maxLen_) \
+  GET_MATCHES_FOOTER_BASE(_maxLen_, GetMatchesSpec1_512)
+#endif
+
 
 #define GET_MATCHES_FOOTER_HC(_maxLen_) \
   GET_MATCHES_FOOTER_BASE(_maxLen_, Hc_GetMatchesSpec)
 
 
 
-#define UPDATE_maxLen { \
+#define UPDATE_maxLen_CPU { \
     const ptrdiff_t diff = (ptrdiff_t)0 - (ptrdiff_t)d2; \
     const Byte *c = cur + maxLen; \
     const Byte *lim = cur + lenLimit; \
     for (; c != lim; c++) if (*(c + diff) != *c) break; \
     maxLen = (unsigned)(c - cur); }
 
+
+// Macro compares bytes at two different positions and returns max length
+#ifdef USE_LZFIND_FUNCTIONS_512
+#define UPDATE_maxLen_512 { \
+    const ptrdiff_t diff = (ptrdiff_t)0 - (ptrdiff_t)d2; \
+    const Byte *c = cur + maxLen; \
+    unsigned long matchValue; \
+    while(maxLen <= lenLimit) {\
+      __m128i pbVector = mm_loadu_epi8((c)); \
+      __m128i curVector = mm_loadu_epi8((c + diff)); \
+      MatchScanForward(&matchValue, mm_cmpare_epi8_mask(pbVector, curVector)); \
+      maxLen = maxLen + (unsigned)matchValue; \
+      if (matchValue != 16) break; \
+      c = c + 16; \
+      } \
+    if(maxLen > lenLimit) { \
+      maxLen = lenLimit; \
+    } \
+}
+#endif
+
 static UInt32* Bt2_MatchFinder_GetMatches(CMatchFinder *p, UInt32 *distances)
 {
   GET_MATCHES_HEADER(2)
   HASH2_CALC
   curMatch = p->hash[hv];
   p->hash[hv] = p->pos;
-  GET_MATCHES_FOOTER_BT(1)
+  GET_MATCHES_FOOTER_BT_CPU(1)
 }
 
 UInt32* Bt3Zip_MatchFinder_GetMatches(CMatchFinder *p, UInt32 *distances)
@@ -1148,7 +1302,7 @@
   HASH_ZIP_CALC
   curMatch = p->hash[hv];
   p->hash[hv] = p->pos;
-  GET_MATCHES_FOOTER_BT(2)
+  GET_MATCHES_FOOTER_BT_CPU(2)
 }
 
 
@@ -1158,7 +1312,7 @@
     mmm = pos;
 
 
-static UInt32* Bt3_MatchFinder_GetMatches(CMatchFinder *p, UInt32 *distances)
+static UInt32* Bt3_MatchFinder_GetMatches_CPU(CMatchFinder *p, UInt32 *distances)
 {
   UInt32 mmm;
   UInt32 h2, d2, pos;
@@ -1174,7 +1328,7 @@
   d2 = pos - hash[h2];
 
   curMatch = (hash + kFix3HashSize)[hv];
-  
+
   hash[h2] = pos;
   (hash + kFix3HashSize)[hv] = pos;
 
@@ -1184,7 +1338,7 @@
 
   if (d2 < mmm && *(cur - d2) == *cur)
   {
-    UPDATE_maxLen
+    UPDATE_maxLen_CPU
     distances[0] = (UInt32)maxLen;
     distances[1] = d2 - 1;
     distances += 2;
@@ -1194,12 +1348,333 @@
       MOVE_POS_RET
     }
   }
+
+  GET_MATCHES_FOOTER_BT_CPU(maxLen)
+}
+
+
+static UInt32* Bt4_MatchFinder_GetMatches_CPU(CMatchFinder *p, UInt32 *distances)
+{
+  UInt32 mmm;
+  UInt32 h2, h3, d2, d3, pos;
+  unsigned maxLen;
+  UInt32 *hash;
+  GET_MATCHES_HEADER(4)
+
+  HASH4_CALC
+
+  hash = p->hash;
+  pos = p->pos;
+
+  d2 = pos - hash                  [h2];
+  d3 = pos - (hash + kFix3HashSize)[h3];
+  curMatch = (hash + kFix4HashSize)[hv];
+
+  hash                  [h2] = pos;
+  (hash + kFix3HashSize)[h3] = pos;
+  (hash + kFix4HashSize)[hv] = pos;
+
+  SET_mmm
+
+  maxLen = 3;
+
+  for (;;)
+  {
+    if (d2 < mmm && *(cur - d2) == *cur)
+    {
+      distances[0] = 2;
+      distances[1] = d2 - 1;
+      distances += 2;
+      if (*(cur - d2 + 2) == cur[2])
+      {
+        // distances[-2] = 3;
+      }
+      else if (d3 < mmm && *(cur - d3) == *cur)
+      {
+        d2 = d3;
+        distances[1] = d3 - 1;
+        distances += 2;
+      }
+      else
+        break;
+    }
+    else if (d3 < mmm && *(cur - d3) == *cur)
+    {
+      d2 = d3;
+      distances[1] = d3 - 1;
+      distances += 2;
+    }
+    else
+      break;
+
+    UPDATE_maxLen_CPU
+    distances[-2] = (UInt32)maxLen;
+    if (maxLen == lenLimit)
+    {
+      SkipMatchesSpec(MF_PARAMS(p));
+      MOVE_POS_RET
+    }
+    break;
+  }
+
+  GET_MATCHES_FOOTER_BT_CPU(maxLen)
+}
+
+
+static UInt32* Bt5_MatchFinder_GetMatches_CPU(CMatchFinder *p, UInt32 *distances)
+{
+  UInt32 mmm;
+  UInt32 h2, h3, d2, d3, maxLen, pos;
+  UInt32 *hash;
+  GET_MATCHES_HEADER(5)
+
+  HASH5_CALC
+
+  hash = p->hash;
+  pos = p->pos;
+
+  d2 = pos - hash                  [h2];
+  d3 = pos - (hash + kFix3HashSize)[h3];
+  // d4 = pos - (hash + kFix4HashSize)[h4];
+
+  curMatch = (hash + kFix5HashSize)[hv];
+
+  hash                  [h2] = pos;
+  (hash + kFix3HashSize)[h3] = pos;
+  // (hash + kFix4HashSize)[h4] = pos;
+  (hash + kFix5HashSize)[hv] = pos;
+
+  SET_mmm
+
+  maxLen = 4;
+
+  for (;;)
+  {
+    if (d2 < mmm && *(cur - d2) == *cur)
+    {
+      distances[0] = 2;
+      distances[1] = d2 - 1;
+      distances += 2;
+      if (*(cur - d2 + 2) == cur[2])
+      {
+      }
+      else if (d3 < mmm && *(cur - d3) == *cur)
+      {
+        distances[1] = d3 - 1;
+        distances += 2;
+        d2 = d3;
+      }
+      else
+        break;
+    }
+    else if (d3 < mmm && *(cur - d3) == *cur)
+    {
+      distances[1] = d3 - 1;
+      distances += 2;
+      d2 = d3;
+    }
+    else
+      break;
+
+    distances[-2] = 3;
+    if (*(cur - d2 + 3) != cur[3])
+      break;
+    UPDATE_maxLen_CPU
+    distances[-2] = (UInt32)maxLen;
+    if (maxLen == lenLimit)
+    {
+      SkipMatchesSpec(MF_PARAMS(p));
+      MOVE_POS_RET
+    }
+    break;
+  }
+
+  GET_MATCHES_FOOTER_BT_CPU(maxLen)
+}
+
+
+static UInt32* Hc4_MatchFinder_GetMatches_CPU(CMatchFinder *p, UInt32 *distances)
+{
+  UInt32 mmm;
+  UInt32 h2, h3, d2, d3, pos;
+  unsigned maxLen;
+  UInt32 *hash;
+  GET_MATCHES_HEADER(4)
+
+  HASH4_CALC
+
+  hash = p->hash;
+  pos = p->pos;
+
+  d2 = pos - hash                  [h2];
+  d3 = pos - (hash + kFix3HashSize)[h3];
+  curMatch = (hash + kFix4HashSize)[hv];
+
+  hash                  [h2] = pos;
+  (hash + kFix3HashSize)[h3] = pos;
+  (hash + kFix4HashSize)[hv] = pos;
+
+  SET_mmm
+
+  maxLen = 3;
+
+  for (;;)
+  {
+    if (d2 < mmm && *(cur - d2) == *cur)
+    {
+      distances[0] = 2;
+      distances[1] = d2 - 1;
+      distances += 2;
+      if (*(cur - d2 + 2) == cur[2])
+      {
+        // distances[-2] = 3;
+      }
+      else if (d3 < mmm && *(cur - d3) == *cur)
+      {
+        d2 = d3;
+        distances[1] = d3 - 1;
+        distances += 2;
+      }
+      else
+        break;
+    }
+    else if (d3 < mmm && *(cur - d3) == *cur)
+    {
+      d2 = d3;
+      distances[1] = d3 - 1;
+      distances += 2;
+    }
+    else
+      break;
+
+    UPDATE_maxLen_CPU
+    distances[-2] = (UInt32)maxLen;
+    if (maxLen == lenLimit)
+    {
+      p->son[p->cyclicBufferPos] = curMatch;
+      MOVE_POS_RET
+    }
+    break;
+  }
   
-  GET_MATCHES_FOOTER_BT(maxLen)
+  GET_MATCHES_FOOTER_HC(maxLen)
+}
+
+
+static UInt32 * Hc5_MatchFinder_GetMatches_CPU(CMatchFinder *p, UInt32 *distances)
+{
+  UInt32 mmm;
+  UInt32 h2, h3, d2, d3, maxLen, pos;
+  UInt32 *hash;
+  GET_MATCHES_HEADER(5)
+
+  HASH5_CALC
+
+  hash = p->hash;
+  pos = p->pos;
+
+  d2 = pos - hash                  [h2];
+  d3 = pos - (hash + kFix3HashSize)[h3];
+  // d4 = pos - (hash + kFix4HashSize)[h4];
+
+  curMatch = (hash + kFix5HashSize)[hv];
+
+  hash                  [h2] = pos;
+  (hash + kFix3HashSize)[h3] = pos;
+  // (hash + kFix4HashSize)[h4] = pos;
+  (hash + kFix5HashSize)[hv] = pos;
+
+  SET_mmm
+
+  maxLen = 4;
+
+  for (;;)
+  {
+    if (d2 < mmm && *(cur - d2) == *cur)
+    {
+      distances[0] = 2;
+      distances[1] = d2 - 1;
+      distances += 2;
+      if (*(cur - d2 + 2) == cur[2])
+      {
+      }
+      else if (d3 < mmm && *(cur - d3) == *cur)
+      {
+        distances[1] = d3 - 1;
+        distances += 2;
+        d2 = d3;
+      }
+      else
+        break;
+    }
+    else if (d3 < mmm && *(cur - d3) == *cur)
+    {
+      distances[1] = d3 - 1;
+      distances += 2;
+      d2 = d3;
+    }
+    else
+      break;
+
+    distances[-2] = 3;
+    if (*(cur - d2 + 3) != cur[3])
+      break;
+    UPDATE_maxLen_CPU
+    distances[-2] = maxLen;
+    if (maxLen == lenLimit)
+    {
+      p->son[p->cyclicBufferPos] = curMatch;
+      MOVE_POS_RET
+    }
+    break;
+  }
+
+  GET_MATCHES_FOOTER_HC(maxLen)
+}
+
+#ifdef USE_LZFIND_FUNCTIONS_512
+static UInt32* Bt3_MatchFinder_GetMatches_512(CMatchFinder *p, UInt32 *distances)
+{
+  UInt32 mmm;
+  UInt32 h2, d2, pos;
+  unsigned maxLen;
+  UInt32 *hash;
+  GET_MATCHES_HEADER(3)
+
+  HASH3_CALC
+
+  hash = p->hash;
+  pos = p->pos;
+
+  d2 = pos - hash[h2];
+
+  curMatch = (hash + kFix3HashSize)[hv];
+
+  hash[h2] = pos;
+  (hash + kFix3HashSize)[hv] = pos;
+
+  SET_mmm
+
+  maxLen = 2;
+
+  if (d2 < mmm && *(cur - d2) == *cur)
+  {
+    UPDATE_maxLen_512
+    distances[0] = (UInt32)maxLen;
+    distances[1] = d2 - 1;
+    distances += 2;
+    if (maxLen == lenLimit)
+    {
+      SkipMatchesSpec(MF_PARAMS(p));
+      MOVE_POS_RET
+    }
+  }
+
+  GET_MATCHES_FOOTER_BT_512(maxLen)
 }
 
 
-static UInt32* Bt4_MatchFinder_GetMatches(CMatchFinder *p, UInt32 *distances)
+static UInt32* Bt4_MatchFinder_GetMatches_512(CMatchFinder *p, UInt32 *distances)
 {
   UInt32 mmm;
   UInt32 h2, h3, d2, d3, pos;
@@ -1253,7 +1728,7 @@
     else
       break;
   
-    UPDATE_maxLen
+    UPDATE_maxLen_512
     distances[-2] = (UInt32)maxLen;
     if (maxLen == lenLimit)
     {
@@ -1262,12 +1737,12 @@
     }
     break;
   }
-  
-  GET_MATCHES_FOOTER_BT(maxLen)
+
+  GET_MATCHES_FOOTER_BT_512(maxLen)
 }
 
 
-static UInt32* Bt5_MatchFinder_GetMatches(CMatchFinder *p, UInt32 *distances)
+static UInt32* Bt5_MatchFinder_GetMatches_512(CMatchFinder *p, UInt32 *distances)
 {
   UInt32 mmm;
   UInt32 h2, h3, d2, d3, maxLen, pos;
@@ -1325,7 +1800,7 @@
     distances[-2] = 3;
     if (*(cur - d2 + 3) != cur[3])
       break;
-    UPDATE_maxLen
+    UPDATE_maxLen_512
     distances[-2] = (UInt32)maxLen;
     if (maxLen == lenLimit)
     {
@@ -1334,12 +1809,12 @@
     }
     break;
   }
-  
-  GET_MATCHES_FOOTER_BT(maxLen)
+
+  GET_MATCHES_FOOTER_BT_512(maxLen)
 }
 
 
-static UInt32* Hc4_MatchFinder_GetMatches(CMatchFinder *p, UInt32 *distances)
+static UInt32* Hc4_MatchFinder_GetMatches_512(CMatchFinder *p, UInt32 *distances)
 {
   UInt32 mmm;
   UInt32 h2, h3, d2, d3, pos;
@@ -1351,7 +1826,7 @@
 
   hash = p->hash;
   pos = p->pos;
-  
+
   d2 = pos - hash                  [h2];
   d3 = pos - (hash + kFix3HashSize)[h3];
   curMatch = (hash + kFix4HashSize)[hv];
@@ -1393,7 +1868,7 @@
     else
       break;
 
-    UPDATE_maxLen
+    UPDATE_maxLen_512
     distances[-2] = (UInt32)maxLen;
     if (maxLen == lenLimit)
     {
@@ -1407,7 +1882,7 @@
 }
 
 
-static UInt32 * Hc5_MatchFinder_GetMatches(CMatchFinder *p, UInt32 *distances)
+static UInt32 * Hc5_MatchFinder_GetMatches_512(CMatchFinder *p, UInt32 *distances)
 {
   UInt32 mmm;
   UInt32 h2, h3, d2, d3, maxLen, pos;
@@ -1431,7 +1906,7 @@
   (hash + kFix5HashSize)[hv] = pos;
 
   SET_mmm
-  
+
   maxLen = 4;
 
   for (;;)
@@ -1465,7 +1940,7 @@
     distances[-2] = 3;
     if (*(cur - d2 + 3) != cur[3])
       break;
-    UPDATE_maxLen
+    UPDATE_maxLen_512
     distances[-2] = maxLen;
     if (maxLen == lenLimit)
     {
@@ -1474,9 +1949,10 @@
     }
     break;
   }
-  
+
   GET_MATCHES_FOOTER_HC(maxLen)
 }
+#endif
 
 
 UInt32* Hc3Zip_MatchFinder_GetMatches(CMatchFinder *p, UInt32 *distances)
@@ -1676,16 +2152,21 @@
 
 void LzFindPrepare(void)
 {
+  MatchFinder_GetMatches_Function f1 = Bt3_MatchFinder_GetMatches_CPU;
+  MatchFinder_GetMatches_Function f2 = Bt4_MatchFinder_GetMatches_CPU;
+  MatchFinder_GetMatches_Function f3 = Bt5_MatchFinder_GetMatches_CPU;
+  MatchFinder_GetMatches_Function f4 = Hc4_MatchFinder_GetMatches_CPU;
+  MatchFinder_GetMatches_Function f5 = Hc5_MatchFinder_GetMatches_CPU;
   #ifndef FORCE_LZFIND_SATUR_SUB_128
-  #ifdef USE_LZFIND_SATUR_SUB_128
-  LZFIND_SATUR_SUB_CODE_FUNC f = NULL;
+  #ifdef USE_LZFIND_FUNCTIONS_128
+  LZFIND_SATUR_SUB_CODE_FUNC f6 = NULL;
   #ifdef MY_CPU_ARM_OR_ARM64
   {
     if (CPU_IsSupported_NEON())
     {
       // #pragma message ("=== LzFind NEON")
       PRF(printf("\n=== LzFind NEON\n"));
-      f = LzFind_SaturSub_128;
+      f6 = LzFind_SaturSub_128;
     }
     // f = 0; // for debug
   }
@@ -1694,21 +2175,39 @@
   {
     // #pragma message ("=== LzFind SSE41")
     PRF(printf("\n=== LzFind SSE41\n"));
-    f = LzFind_SaturSub_128;
+    f6 = LzFind_SaturSub_128;
 
-    #ifdef USE_LZFIND_SATUR_SUB_256
+    #ifdef USE_LZFIND_FUNCTIONS_256
     if (CPU_IsSupported_AVX2())
     {
       // #pragma message ("=== LzFind AVX2")
       PRF(printf("\n=== LzFind AVX2\n"));
-      f = LzFind_SaturSub_256;
+      f6 = LzFind_SaturSub_256;
     }
     #endif
   }
   #endif // MY_CPU_ARM_OR_ARM64
-  g_LzFind_SaturSub = f;
-  #endif // USE_LZFIND_SATUR_SUB_128
+  g_LzFind_SaturSub = f6;
+  #endif // USE_LZFIND_FUNCTIONS_128
   #endif // FORCE_LZFIND_SATUR_SUB_128
+  #ifndef MY_CPU_ARM_OR_ARM64
+    #ifdef USE_LZFIND_FUNCTIONS_512
+    if (CPU_IsSupported_AVX512())
+    {
+      PRF(printf("\n=== LzFind AVX512\n"));
+      f1 = Bt3_MatchFinder_GetMatches_512;
+      f2 = Bt4_MatchFinder_GetMatches_512;
+      f3 = Bt5_MatchFinder_GetMatches_512;
+      f4 = Hc4_MatchFinder_GetMatches_512;
+      f5 = Hc5_MatchFinder_GetMatches_512;
+    }
+    #endif
+  #endif
+  Bt3_MatchFinder_GetMatches = f1;
+  Bt4_MatchFinder_GetMatches = f2;
+  Bt5_MatchFinder_GetMatches = f3;
+  Hc4_MatchFinder_GetMatches = f4;
+  Hc5_MatchFinder_GetMatches = f5;
 }
 
 
diff -ruN 7zip-Dark7zip/C/LzFindMt.c 7zip-Dark7zip2/C/LzFindMt.c
--- 7zip-Dark7zip/C/LzFindMt.c	2024-01-19 10:53:03.209213101 -0800
+++ 7zip-Dark7zip2/C/LzFindMt.c	2024-01-19 10:53:47.639213589 -0800
@@ -31,6 +31,8 @@
 #define LOG_ITER(x)
 #endif
 
+// #define kEmptyHashValue 0
+
 #define kMtHashBlockSize ((UInt32)1 << 17)
 #define kMtHashNumBlocks (1 << 1)
 
@@ -77,7 +79,12 @@
   h4 = (temp ^ (p->crc[cur[3]] << kLzHash_CrcShift_1)) & p->hash4Mask; }
   // (kHash4Size - 1);
 */
+typedef UInt32 * ( Z7_FASTCALL *GetMatchesSpecN_2Function) (const Byte *lenLimit, size_t pos, const Byte *cur, CLzRef *son,
+    UInt32 _cutValue, UInt32 *d, size_t _maxLen, const UInt32 *hash, const UInt32 *limit, const UInt32 *size,
+    size_t _cyclicBufferPos, UInt32 _cyclicBufferSize,
+    UInt32 *posRes);
 
+static GetMatchesSpecN_2Function GetMatchesSpecN_2;
 
 Z7_NO_INLINE
 static void MtSync_Construct(CMtSync *p)
@@ -552,6 +559,87 @@
 
 #define MFMT_GM_INLINE
 
+#ifndef MFMT_GM_INLINE
+
+#ifdef Z7_SHOW_HW_STATUS
+#include <stdio.h>
+#define PRF(x) x
+PRF(;)
+#else
+#define PRF(x)
+#endif
+
+Z7_FORCE_INLINE
+static UInt32 * GetMatchesSpec1(UInt32 lenLimit, UInt32 curMatch, UInt32 pos, const Byte *cur, CLzRef *son,
+    size_t _cyclicBufferPos, UInt32 _cyclicBufferSize, UInt32 cutValue,
+    UInt32 *d, UInt32 maxLen)
+{
+  CLzRef *ptr0 = son + ((size_t)_cyclicBufferPos << 1) + 1;
+  CLzRef *ptr1 = son + ((size_t)_cyclicBufferPos << 1);
+  unsigned len0 = 0, len1 = 0;
+
+  UInt32 cmCheck;
+
+  // if (curMatch >= pos) { *ptr0 = *ptr1 = kEmptyHashValue; return NULL; }
+
+  cmCheck = (UInt32)(pos - _cyclicBufferSize);
+  if ((UInt32)pos <= _cyclicBufferSize)
+    cmCheck = 0;
+
+  if (cmCheck < curMatch)
+  do
+  {
+    const UInt32 delta = pos - curMatch;
+    {
+      CLzRef *pair = son + ((size_t)(_cyclicBufferPos - delta + ((delta > _cyclicBufferPos) ? _cyclicBufferSize : 0)) << 1);
+      const Byte *pb = cur - delta;
+      unsigned len = (len0 < len1 ? len0 : len1);
+      const UInt32 pair0 = pair[0];
+      if (pb[len] == cur[len])
+      {
+        if (++len != lenLimit && pb[len] == cur[len]){
+          while (++len != lenLimit)
+            if (pb[len] != cur[len])
+              break;
+        }
+        if (maxLen < len)
+        {
+          maxLen = (UInt32)len;
+          *d++ = (UInt32)len;
+          *d++ = delta - 1;
+          if (len == lenLimit)
+          {
+            *ptr1 = pair0;
+            *ptr0 = pair[1];
+            return d;
+          }
+        }
+      }
+      if (pb[len] < cur[len])
+      {
+        *ptr1 = curMatch;
+        // const UInt32 curMatch2 = pair[1];
+        // if (curMatch2 >= curMatch) { *ptr0 = *ptr1 = kEmptyHashValue;  return NULL; }
+        // curMatch = curMatch2;
+        curMatch = pair[1];
+        ptr1 = pair + 1;
+        len1 = len;
+      }
+      else
+      {
+        *ptr0 = curMatch;
+        curMatch = pair[0];
+        ptr0 = pair;
+        len0 = len;
+      }
+    }
+  }
+  while(--cutValue && cmCheck < curMatch);
+
+  *ptr0 = *ptr1 = kEmptyHashValue;
+  return d;
+}
+#endif
 #ifdef MFMT_GM_INLINE
 
 /*
@@ -559,11 +647,17 @@
   to eliminate "movsx" BUG in old MSVC x64 compiler.
 */
 
+UInt32 * Z7_FASTCALL GetMatchesSpecN_2_CPU (const Byte *lenLimit, size_t pos, const Byte *cur, CLzRef *son,
+    UInt32 _cutValue, UInt32 *d, size_t _maxLen, const UInt32 *hash, const UInt32 *limit, const UInt32 *size,
+    size_t _cyclicBufferPos, UInt32 _cyclicBufferSize,
+    UInt32 *posRes);
 
-UInt32 * Z7_FASTCALL GetMatchesSpecN_2(const Byte *lenLimit, size_t pos, const Byte *cur, CLzRef *son,
+#ifdef USE_LZFINDMT_FUNCTIONS_512
+UInt32 * Z7_FASTCALL GetMatchesSpecN_2_512 (const Byte *lenLimit, size_t pos, const Byte *cur, CLzRef *son,
     UInt32 _cutValue, UInt32 *d, size_t _maxLen, const UInt32 *hash, const UInt32 *limit, const UInt32 *size,
     size_t _cyclicBufferPos, UInt32 _cyclicBufferSize,
     UInt32 *posRes);
+#endif
 
 #endif
 
@@ -1398,6 +1492,21 @@
   }
 }
 
+void LzFindMtPrepare(void)
+{
+  GetMatchesSpecN_2Function f2 = GetMatchesSpecN_2_CPU;
+  #ifndef MY_CPU_ARM_OR_ARM64
+    #ifdef USE_LZFINDMT_FUNCTIONS_512
+    if (CPU_IsSupported_AVX512())
+    {
+      PRF(printf("\n=== LzFind AVX512\n"));
+      f2 = GetMatchesSpecN_2_512;
+    }
+    #endif
+  #endif
+  GetMatchesSpecN_2 = f2;
+}
+
 #undef RINOK_THREAD
 #undef PRF
 #undef MF
diff -ruN 7zip-Dark7zip/C/LzFindMt.h 7zip-Dark7zip2/C/LzFindMt.h
--- 7zip-Dark7zip/C/LzFindMt.h	2024-01-19 10:53:03.209213101 -0800
+++ 7zip-Dark7zip2/C/LzFindMt.h	2024-01-19 10:53:47.639213589 -0800
@@ -104,6 +104,45 @@
 SRes MatchFinderMt_InitMt(CMatchFinderMt *p);
 void MatchFinderMt_ReleaseStream(CMatchFinderMt *p);
 
+void LzFindMtPrepare(void);
+
+#ifdef MY_CPU_X86_OR_AMD64
+  #if defined(__clang__) && (__clang_major__ >= 4) \
+    || defined(Z7_GCC_VERSION) && (Z7_GCC_VERSION >= 40701)
+    // || defined(__INTEL_COMPILER) && (__INTEL_COMPILER >= 1900)
+      #define USE_LZFINDMT_FUNCTIONS_128
+      #define USE_LZFINDMT_FUNCTIONS_256
+      #define LZFIND_ATTRIB_SSE41 __attribute__((__target__("sse4.1")))
+      #define LZFIND_ATTRIB_AVX2  __attribute__((__target__("avx2")))
+      #if defined(__clang__) && (__clang_major__ >= 4) \
+        || defined(Z7_GCC_VERSION) && (Z7_GCC_VERSION >= 50000)
+          #define USE_LZFINDMT_FUNCTIONS_512
+      #endif
+  #elif defined(_MSC_VER)
+    #if (_MSC_VER >= 1600)
+      #define USE_LZFINDMT_FUNCTIONS_128
+    #endif
+    #if (_MSC_VER >= 1900)
+      #define USE_LZFINDMT_FUNCTIONS_256
+    #endif
+    #if (_MSC_VER >= 1920)
+      #define USE_LZFINDMT_FUNCTIONS_512
+    #endif
+  #endif
+#endif
+
+UInt32 * Z7_FASTCALL GetMatchesSpecN_2_CPU(const Byte *lenLimit, size_t pos, const Byte *cur, CLzRef *son,
+    UInt32 _cutValue, UInt32 *d, size_t _maxLen, const UInt32 *hash, const UInt32 *limit, const UInt32 *size,
+    size_t _cyclicBufferPos, UInt32 _cyclicBufferSize,
+    UInt32 *posRes);
+
+#ifdef USE_LZFINDMT_FUNCTIONS_512
+UInt32 * Z7_FASTCALL GetMatchesSpecN_2_512(const Byte *lenLimit, size_t pos, const Byte *cur, CLzRef *son,
+    UInt32 _cutValue, UInt32 *d, size_t _maxLen, const UInt32 *hash, const UInt32 *limit, const UInt32 *size,
+    size_t _cyclicBufferPos, UInt32 _cyclicBufferSize,
+    UInt32 *posRes);
+#endif
+
 EXTERN_C_END
 
 #endif
diff -ruN 7zip-Dark7zip/C/LzFindOpt.c 7zip-Dark7zip2/C/LzFindOpt.c
--- 7zip-Dark7zip/C/LzFindOpt.c	2024-01-19 10:53:03.209213101 -0800
+++ 7zip-Dark7zip2/C/LzFindOpt.c	2024-01-22 06:37:46.421888903 -0800
@@ -6,7 +6,7 @@
 #include "CpuArch.h"
 #include "LzFind.h"
 
-// #include "LzFindMt.h"
+#include "LzFindMt.h"
 
 // #define LOG_ITERS
 
@@ -36,6 +36,38 @@
 
 #define kEmptyHashValue 0
 
+#ifdef USE_LZFINDMT_FUNCTIONS_512
+
+#ifdef _MSC_VER
+#include <intrin.h>
+#else
+#include <immintrin.h>
+#endif
+
+#ifdef __GNUC__
+#define MatchScanForward(x, m) *(x) = (unsigned long)__builtin_ctz(m)
+#elif _MSC_VER
+/* BitScanForward is Visual Studio specific. */
+#define MatchScanForward(x,m) _BitScanForward(x,m)
+#endif
+
+#if (defined(__AVX512VL__) && defined(__AVX512BW__))
+  #define mm_cmpare_epi8_mask(x,y) ((_mm_cmpneq_epi8_mask( x, y )) | 0x10000)
+#else
+  #define mm_cmpare_epi8_mask(x,y) (~(_mm_movemask_epi8( _mm_cmpeq_epi8( x, y ))) | 0x10000)
+#endif
+
+#if (defined(__GNUC__) && (__GNUC__ <= 10) && !defined(__clang__))
+  #define mm_loadu_epi8(x) _mm_loadu_si128((__m128i*)(x))
+#else
+  #if (defined(__AVX512VL__) && defined(__AVX512BW__))
+    #define mm_loadu_epi8(x) _mm_loadu_epi8(x)
+  #else
+    #define mm_loadu_epi8(x) _mm_loadu_si128((__m128i*)(x))
+  #endif
+#endif
+#endif
+
 // #define CYC_TO_POS_OFFSET 0
 
 // #define CYC_TO_POS_OFFSET 1 // for debug
@@ -214,13 +246,8 @@
   to eliminate "movsx" BUG in old MSVC x64 compiler.
 */
 
-UInt32 * Z7_FASTCALL GetMatchesSpecN_2(const Byte *lenLimit, size_t pos, const Byte *cur, CLzRef *son,
-    UInt32 _cutValue, UInt32 *d, size_t _maxLen, const UInt32 *hash, const UInt32 *limit, const UInt32 *size,
-    size_t _cyclicBufferPos, UInt32 _cyclicBufferSize,
-    UInt32 *posRes);
-
 Z7_NO_INLINE
-UInt32 * Z7_FASTCALL GetMatchesSpecN_2(const Byte *lenLimit, size_t pos, const Byte *cur, CLzRef *son,
+UInt32 * Z7_FASTCALL GetMatchesSpecN_2_CPU(const Byte *lenLimit, size_t pos, const Byte *cur, CLzRef *son,
     UInt32 _cutValue, UInt32 *d, size_t _maxLen, const UInt32 *hash, const UInt32 *limit, const UInt32 *size,
     size_t _cyclicBufferPos, UInt32 _cyclicBufferSize,
     UInt32 *posRes)
@@ -399,7 +426,198 @@
   return d;
 }
 
+#ifdef USE_LZFINDMT_FUNCTIONS_512
+Z7_NO_INLINE
+UInt32 * Z7_FASTCALL GetMatchesSpecN_2_512(const Byte *lenLimit, size_t pos, const Byte *cur, CLzRef *son,
+    UInt32 _cutValue, UInt32 *d, size_t _maxLen, const UInt32 *hash, const UInt32 *limit, const UInt32 *size,
+    size_t _cyclicBufferPos, UInt32 _cyclicBufferSize,
+    UInt32 *posRes)
+{
+  do // while (hash != size)
+  {
+    UInt32 delta;
+
+  #ifndef cbs
+    UInt32 cbs;
+  #endif
+
+    if (hash == size)
+      break;
+
+    delta = *hash++;
+
+    if (delta == 0)
+      return NULL;
+
+    lenLimit++;
+
+  #ifndef cbs
+    cbs = _cyclicBufferSize;
+    if ((UInt32)pos < cbs)
+    {
+      if (delta > (UInt32)pos)
+        return NULL;
+      cbs = (UInt32)pos;
+    }
+  #endif
+
+    if (delta >= cbs)
+    {
+      CLzRef *ptr1 = son + ((size_t)_cyclicBufferPos << 1);
+      *d++ = 0;
+      ptr1[0] = kEmptyHashValue;
+      ptr1[1] = kEmptyHashValue;
+    }
+else
+{
+  UInt32 *_distances = ++d;
+
+  CLzRef *ptr0 = son + ((size_t)_cyclicBufferPos << 1) + 1;
+  CLzRef *ptr1 = son + ((size_t)_cyclicBufferPos << 1);
+
+  UInt32 cutValue = _cutValue;
+  const Byte *len0 = cur, *len1 = cur;
+  const Byte *maxLen = cur + _maxLen;
+
+  // if (cutValue == 0) { *ptr0 = *ptr1 = kEmptyHashValue; } else
+  for (LOG_ITER(g_NumIters_Tree++);;)
+  {
+    LOG_ITER(g_NumIters_Loop++);
+    {
+      // SPEC code
+      CLzRef *pair = son + ((size_t)((ptrdiff_t)_cyclicBufferPos - (ptrdiff_t)delta
+          + (ptrdiff_t)(UInt32)(_cyclicBufferPos < delta ? cbs : 0)
+          ) << 1);
+
+      const ptrdiff_t diff = (ptrdiff_t)0 - (ptrdiff_t)delta;
+      const Byte *len = (len0 < len1 ? len0 : len1);
+
+    #ifdef USE_SON_PREFETCH
+      const UInt32 pair0 = *pair;
+    #endif
+
+      unsigned long matchValue = 16;
+      if (len[diff] == len[0])
+      {
+        if (++len != lenLimit && len[diff] == len[0])
+        {
+          while (len != lenLimit)
+          {
+            LOG_ITER(g_NumIters_Bytes + matchValue);
+            __m128i pbVector = mm_loadu_epi8(len + diff);
+            __m128i curVector = mm_loadu_epi8(len + 0);
+            MatchScanForward(&matchValue, mm_cmpare_epi8_mask(pbVector, curVector));
+            len = len + (unsigned)matchValue;
+            if (matchValue != 16)
+              break;
+          }
+          if(len >= lenLimit)
+          {
+            len = lenLimit;
+          }
+        }
+        if (maxLen < len)
+        {
+          maxLen = len;
+          *d++ = (UInt32)(len - cur);
+          *d++ = delta - 1;
+
+          if (len == lenLimit)
+          {
+            const UInt32 pair1 = pair[1];
+            *ptr1 =
+              #ifdef USE_SON_PREFETCH
+                pair0;
+              #else
+                pair[0];
+              #endif
+            *ptr0 = pair1;
+
+            _distances[-1] = (UInt32)(d - _distances);
+
+            #ifdef USE_LONG_MATCH_OPT
+
+                if (hash == size || *hash != delta || lenLimit[diff] != lenLimit[0] || d >= limit)
+                  break;
+
+            {
+              for (;;)
+              {
+                *d++ = 2;
+                *d++ = (UInt32)(lenLimit - cur);
+                *d++ = delta - 1;
+                cur++;
+                lenLimit++;
+                // SPEC
+                _cyclicBufferPos++;
+                {
+                  // SPEC code
+                  CLzRef *dest = son + ((size_t)(_cyclicBufferPos) << 1);
+                  const CLzRef *src = dest + ((diff
+                      + (ptrdiff_t)(UInt32)((_cyclicBufferPos < delta) ? cbs : 0)) << 1);
+                  // CLzRef *ptr = son + ((size_t)(pos) << 1) - CYC_TO_POS_OFFSET * 2;
+                  #if 0
+                  *(UInt64 *)(void *)dest = *((const UInt64 *)(const void *)src);
+                  #else
+                  const UInt32 p0 = src[0];
+                  const UInt32 p1 = src[1];
+                  dest[0] = p0;
+                  dest[1] = p1;
+                  #endif
+                }
+                pos++;
+                hash++;
+                if (hash == size || *hash != delta || lenLimit[diff] != lenLimit[0] || d >= limit)
+                  break;
+              } // for() end for long matches
+            }
+            #endif
+
+            break; // break from TREE iterations
+          }
+        }
+      }
+      {
+        const UInt32 curMatch = (UInt32)pos - delta; // (UInt32)(pos + diff);
+        if (len[diff] < len[0])
+        {
+          delta = pair[1];
+          *ptr1 = curMatch;
+          ptr1 = pair + 1;
+          len1 = len;
+          if (delta >= curMatch)
+            return NULL;
+        }
+        else
+        {
+          delta = *pair;
+          *ptr0 = curMatch;
+          ptr0 = pair;
+          len0 = len;
+          if (delta >= curMatch)
+            return NULL;
+        }
+        delta = (UInt32)pos - delta;
 
+        if (--cutValue == 0 || delta >= cbs)
+        {
+          *ptr0 = *ptr1 = kEmptyHashValue;
+          _distances[-1] = (UInt32)(d - _distances);
+          break;
+        }
+      }
+    }
+  } // for (tree iterations)
+}
+    pos++;
+    _cyclicBufferPos++;
+    cur++;
+  }
+  while (d < limit);
+  *posRes = (UInt32)pos;
+  return d;
+}
+#endif
 
 /*
 typedef UInt32 uint32plus; // size_t
diff -ruN 7zip-Dark7zip/C/LzmaEnc.c 7zip-Dark7zip2/C/LzmaEnc.c
--- 7zip-Dark7zip/C/LzmaEnc.c	2024-01-19 10:53:03.209213101 -0800
+++ 7zip-Dark7zip2/C/LzmaEnc.c	2024-01-22 06:37:46.421888903 -0800
@@ -1075,8 +1075,138 @@
     p->additionalOffset += (num); \
     p->matchFinder.Skip(p->matchFinderObj, (UInt32)(num)); }
 
+// #define Z7_SHOW_HW_STATUS
 
-static unsigned ReadMatchDistances(CLzmaEnc *p, unsigned *numPairsRes)
+#ifdef Z7_SHOW_HW_STATUS
+#include <stdio.h>
+#define PRF(x) x
+PRF(;)
+#else
+#define PRF(x)
+#endif
+
+#ifdef MY_CPU_X86_OR_AMD64
+  #if defined(__clang__) && (__clang_major__ >= 4) \
+    || defined(Z7_GCC_VERSION) && (Z7_GCC_VERSION >= 50000)
+    // || defined(__INTEL_COMPILER) && (__INTEL_COMPILER >= 1900)
+      #define USE_LZMAENC_FUNCTIONS_512
+  #elif defined(_MSC_VER)
+    #if (_MSC_VER >= 1920)
+      #define USE_LZMAENC_FUNCTIONS_512
+    #endif
+  #endif
+
+// #elif defined(MY_CPU_ARM_OR_ARM64)
+#elif defined(MY_CPU_ARM64)
+
+  #if defined(__clang__) && (__clang_major__ >= 8) \
+    || defined(__GNUC__) && (__GNUC__ >= 8)
+      #define USE_LZMAENC_FUNCTIONS_128
+    #ifdef MY_CPU_ARM64
+      // #define LZMAENC_ATTRIB_SSE41 __attribute__((__target__("")))
+    #else
+      // #define LZMAENC_ATTRIB_SSE41 __attribute__((__target__("fpu=crypto-neon-fp-armv8")))
+    #endif
+
+  #elif defined(_MSC_VER)
+    #if (_MSC_VER >= 1910)
+      #define USE_LZMAENC_FUNCTIONS_128
+    #endif
+  #endif
+
+  #if defined(_MSC_VER) && defined(MY_CPU_ARM64)
+    #include <arm64_neon.h>
+  #else
+    #include <arm_neon.h>
+  #endif
+
+#endif
+
+#ifdef USE_LZMAENC_FUNCTIONS_512
+
+#ifdef _MSC_VER
+#include <intrin.h>
+#else
+#include <immintrin.h>
+#endif
+
+#ifdef __GNUC__
+#define MatchScanForward(x, m) *(x) = (unsigned long)__builtin_ctz(m)
+#elif _MSC_VER
+/* BitScanForward is Visual Studio specific. */
+#define MatchScanForward(x,m) _BitScanForward(x,m)
+#endif
+
+#if (defined(__AVX512VL__) && defined(__AVX512BW__))
+  #define mm_cmpare_epi8_mask(x,y) ((_mm_cmpneq_epi8_mask( x, y )) | 0x10000)
+#else
+  #define mm_cmpare_epi8_mask(x,y) (~(_mm_movemask_epi8( _mm_cmpeq_epi8( x, y ))) | 0x10000)
+#endif
+
+#if (defined(__GNUC__) && (__GNUC__ <= 10) && !defined(__clang__))
+  #define mm_loadu_epi8(x) _mm_loadu_si128((__m128i*)(x))
+#else
+  #if (defined(__AVX512VL__) && defined(__AVX512BW__))
+    #define mm_loadu_epi8(x) _mm_loadu_epi8(x)
+  #else
+    #define mm_loadu_epi8(x) _mm_loadu_si128((__m128i*)(x))
+  #endif
+#endif
+#endif
+
+typedef unsigned (*ReadMatchDistancesFunction) (CLzmaEnc *p, unsigned *numPairsRes);
+
+static ReadMatchDistancesFunction ReadMatchDistances;
+
+static unsigned ReadMatchDistances_CPU(CLzmaEnc *p, unsigned *numPairsRes)
+{
+  unsigned numPairs;
+
+  p->additionalOffset++;
+  p->numAvail = p->matchFinder.GetNumAvailableBytes(p->matchFinderObj);
+  {
+    const UInt32 *d = p->matchFinder.GetMatches(p->matchFinderObj, p->matches);
+    // if (!d) { p->mf_Failure = True; *numPairsRes = 0;  return 0; }
+    numPairs = (unsigned)(d - p->matches);
+  }
+  *numPairsRes = numPairs;
+
+  #ifdef SHOW_STAT
+  printf("\n i = %u numPairs = %u    ", g_STAT_OFFSET, numPairs / 2);
+  g_STAT_OFFSET++;
+  {
+    unsigned i;
+    for (i = 0; i < numPairs; i += 2)
+      printf("%2u %6u   | ", p->matches[i], p->matches[i + 1]);
+  }
+  #endif
+
+  if (numPairs == 0)
+    return 0;
+  {
+    const unsigned len = p->matches[(size_t)numPairs - 2];
+    if (len != p->numFastBytes)
+      return len;
+    {
+      UInt32 numAvail = p->numAvail;
+      if (numAvail > LZMA_MATCH_LEN_MAX)
+        numAvail = LZMA_MATCH_LEN_MAX;
+      {
+        const Byte *p1 = p->matchFinder.GetPointerToCurrentPos(p->matchFinderObj) - 1;
+        const Byte *p2 = p1 + len;
+        const ptrdiff_t dif = (ptrdiff_t)-1 - (ptrdiff_t)p->matches[(size_t)numPairs - 1];
+        const Byte *lim = p1 + numAvail;
+
+        for (; p2 != lim && *p2 == p2[dif]; p2++)
+        {}
+        return (unsigned)(p2 - p1);
+      }
+    }
+  }
+}
+
+#ifdef USE_LZMAENC_FUNCTIONS_512
+static unsigned ReadMatchDistances_512(CLzmaEnc *p, unsigned *numPairsRes)
 {
   unsigned numPairs;
   
@@ -1114,13 +1244,25 @@
         const Byte *p2 = p1 + len;
         const ptrdiff_t dif = (ptrdiff_t)-1 - (ptrdiff_t)p->matches[(size_t)numPairs - 1];
         const Byte *lim = p1 + numAvail;
-        for (; p2 != lim && *p2 == p2[dif]; p2++)
-        {}
+
+        unsigned long matchVector = 16;
+        while(p2 != lim)
+        {
+          __m128i pbVector = mm_loadu_epi8(p2);
+          __m128i curVector = mm_loadu_epi8(p2 + dif);
+          MatchScanForward(&matchVector, mm_cmpare_epi8_mask(pbVector, curVector));
+          p2 = p2 + matchVector;
+          if (matchVector != 16) break;
+        }
+        if(p2 >= lim) {
+          p2 =lim;
+        }
         return (unsigned)(p2 - p1);
       }
     }
   }
 }
+#endif
 
 #define MARK_LIT ((UInt32)(Int32)-1)
 
@@ -1680,7 +1822,7 @@
           limit = numAvailFull;
         for (len = 3; len < limit && data[len] == data2[len]; len++)
         {}
-        
+
         {
           unsigned state2 = kLiteralNextStates[state];
           unsigned posState2 = (position + 1) & p->pbMask;
@@ -3131,6 +3273,21 @@
   return res;
 }
 
+void LzmaEncodeFunctionsPrepare(void)
+{
+  ReadMatchDistancesFunction f = ReadMatchDistances_CPU;
+  #ifdef USE_LZMAENC_FUNCTIONS_512
+  #ifndef MY_CPU_ARM_OR_ARM64
+  if (CPU_IsSupported_AVX512())
+  {
+        PRF(printf("\n=== LzmaEnc AVX512\n"));
+        f = ReadMatchDistances_512;
+  }
+  #endif // MY_CPU_ARM_OR_ARM64
+  #endif // USE_LZMAENC_FUNCTIONS_512
+  ReadMatchDistances = f;
+}
+
 
 /*
 #ifndef Z7_ST
diff -ruN 7zip-Dark7zip/C/LzmaEnc.h 7zip-Dark7zip2/C/LzmaEnc.h
--- 7zip-Dark7zip/C/LzmaEnc.h	2024-01-19 10:53:03.209213101 -0800
+++ 7zip-Dark7zip2/C/LzmaEnc.h	2024-01-19 10:53:47.639213589 -0800
@@ -78,6 +78,8 @@
     const CLzmaEncProps *props, Byte *propsEncoded, SizeT *propsSize, int writeEndMark,
     ICompressProgressPtr progress, ISzAllocPtr alloc, ISzAllocPtr allocBig);
 
+void LzmaEncodeFunctionsPrepare(void);
+
 EXTERN_C_END
 
 #endif
diff -ruN 7zip-Dark7zip/CPP/7zip/7zip_gcc.mak 7zip-Dark7zip2/CPP/7zip/7zip_gcc.mak
--- 7zip-Dark7zip/CPP/7zip/7zip_gcc.mak	2024-01-19 10:53:03.209213101 -0800
+++ 7zip-Dark7zip2/CPP/7zip/7zip_gcc.mak	2024-01-19 10:53:47.639213589 -0800
@@ -1,7 +1,7 @@
 # USE_CLANG=1
 # USE_ASM = 1
 # IS_X64 = 1
-# MY_ARCH =
+MY_ARCH = -march=native
 # USE_ASM=
 # USE_JWASM=1
 
diff -ruN 7zip-Dark7zip/CPP/7zip/Bundles/Alone2/makefile 7zip-Dark7zip2/CPP/7zip/Bundles/Alone2/makefile
--- 7zip-Dark7zip/CPP/7zip/Bundles/Alone2/makefile	2024-01-19 10:53:03.209213101 -0800
+++ 7zip-Dark7zip2/CPP/7zip/Bundles/Alone2/makefile	2024-01-19 10:54:25.619214006 -0800
@@ -1,6 +1,14 @@
 PROG = 7zz.exe
 # USE_C_AES = 1
 # USE_C_SHA = 1
+# USE_C_LZFINDOPT flag to be enabled to run C version of code in Windows with MSVC.
+# USE_C_LZFINDOPT = 1
+#arch:AVX512 to be added in machines with AVX512 support for performance gains
+# Check if the flag is set
+!if "$(USE_AVX512)" == "1"
+CFLAGS = /arch:AVX512
+!endif
+
 CFLAGS = $(CFLAGS) -DZ7_PROG_VARIANT_Z
 
 !include "../Format7zF/Arc.mak"
diff -ruN 7zip-Dark7zip/CPP/7zip/Crypto/MyAes.cpp 7zip-Dark7zip2/CPP/7zip/Crypto/MyAes.cpp
--- 7zip-Dark7zip/CPP/7zip/Crypto/MyAes.cpp	2024-01-19 10:53:03.209213101 -0800
+++ 7zip-Dark7zip2/CPP/7zip/Crypto/MyAes.cpp	2024-01-19 10:53:47.659213589 -0800
@@ -175,10 +175,15 @@
       if (algo == 2) if (g_Aes_SupportedFunctions_Flags & k_Aes_SupportedFunctions_HW) \
       { f = f2; }
   #ifdef MY_CPU_X86_OR_AMD64
-    #define SET_AES_FUNC_23(f2, f3) \
+    #define SET_AES_FUNC_23(f2, f3, f4) \
       SET_AES_FUNC_2(f2) \
-      if (algo == 3) if (g_Aes_SupportedFunctions_Flags & k_Aes_SupportedFunctions_HW_256) \
-      { f = f3; }
+      if (algo == 3) \
+      { \
+        if (g_Aes_SupportedFunctions_Flags & k_Aes_SupportedFunctions_HW_256) \
+        { f = f3; } \
+        if (g_Aes_SupportedFunctions_Flags & k_Aes_SupportedFunctions_HW_512) \
+        { f = f4; } \
+      }
   #else  // MY_CPU_X86_OR_AMD64
     #define SET_AES_FUNC_23(f2, f3) \
       SET_AES_FUNC_2(f2)
@@ -187,12 +192,12 @@
     #define SET_AES_FUNC_23(f2, f3)
 #endif // USE_HW_AES
 
-#define SET_AES_FUNCS(c, f0, f1, f2, f3) \
+#define SET_AES_FUNCS(c, f0, f1, f2, f3, f4) \
   bool c::SetFunctions(UInt32 algo) { \
   _codeFunc = f0; if (algo < 1) return true; \
   AES_CODE_FUNC f = NULL; \
   if (algo == 1) { f = f1; } \
-  SET_AES_FUNC_23(f2, f3) \
+  SET_AES_FUNC_23(f2, f3, f4) \
   if (f) { _codeFunc = f; return true; } \
   return false; }
 
@@ -204,6 +209,7 @@
   g_AesCtr_Code,
     AesCtr_Code,
     AesCtr_Code_HW,
+    AesCtr_Code_HW_256,
     AesCtr_Code_HW_256)
 #endif
 
@@ -212,6 +218,7 @@
   g_AesCbc_Encode,
     AesCbc_Encode,
     AesCbc_Encode_HW,
+    AesCbc_Encode_HW,
     AesCbc_Encode_HW)
 
 SET_AES_FUNCS(
@@ -219,7 +226,8 @@
   g_AesCbc_Decode,
     AesCbc_Decode,
     AesCbc_Decode_HW,
-    AesCbc_Decode_HW_256)
+    AesCbc_Decode_HW_256,
+    AesCbc_Decode_HW_512)
 
 Z7_COM7F_IMF(CAesCoder::SetCoderProperties(const PROPID *propIDs, const PROPVARIANT *coderProps, UInt32 numProps))
 {
diff -ruN 7zip-Dark7zip/CPP/7zip/UI/Console/MainAr.cpp 7zip-Dark7zip2/CPP/7zip/UI/Console/MainAr.cpp
--- 7zip-Dark7zip/CPP/7zip/UI/Console/MainAr.cpp	2024-01-19 10:53:03.209213101 -0800
+++ 7zip-Dark7zip2/CPP/7zip/UI/Console/MainAr.cpp	2024-01-19 10:53:47.669213589 -0800
@@ -61,7 +61,10 @@
 {
   // __try
   {
-    #if defined(__AVX2__)
+    #if defined(__AVX512F__)
+      if (!CPU_IsSupported_AVX512())
+        return false;
+    #elif defined(__AVX2__)
       if (!CPU_IsSupported_AVX2())
         return false;
     #elif defined(__AVX__)
diff -ruN 7zip-Dark7zip/CPP/Common/LzFindPrepare.cpp 7zip-Dark7zip2/CPP/Common/LzFindPrepare.cpp
--- 7zip-Dark7zip/CPP/Common/LzFindPrepare.cpp	2024-01-19 10:53:03.209213101 -0800
+++ 7zip-Dark7zip2/CPP/Common/LzFindPrepare.cpp	2024-01-19 10:53:47.679213589 -0800
@@ -3,5 +3,7 @@
 #include "StdAfx.h"
 
 #include "../../C/LzFind.h"
+#include "../../C/LzmaEnc.h"
+#include "../../C/LzFindMt.h"
 
-static struct CLzFindPrepare { CLzFindPrepare() { LzFindPrepare(); } } g_CLzFindPrepare;
+static struct CLzFindPrepare { CLzFindPrepare() { LzFindPrepare(); LzmaEncodeFunctionsPrepare(); LzFindMtPrepare(); } } g_CLzFindPrepare;
diff -ruN 7zip-Dark7zip/CPP/Windows/SystemInfo.cpp 7zip-Dark7zip2/CPP/Windows/SystemInfo.cpp
--- 7zip-Dark7zip/CPP/Windows/SystemInfo.cpp	2024-01-19 10:53:03.209213101 -0800
+++ 7zip-Dark7zip2/CPP/Windows/SystemInfo.cpp	2024-01-19 10:53:47.679213589 -0800
@@ -990,7 +990,9 @@
     s.Add_UInt32(_MSC_VER);
   #endif
 
-    #if defined(__AVX2__)
+    #if defined(__AVX512F__)
+      #define MY_CPU_COMPILE_ISA "AVX512"
+    #elif defined(__AVX2__)
       #define MY_CPU_COMPILE_ISA "AVX2"
     #elif defined(__AVX__)
       #define MY_CPU_COMPILE_ISA "AVX"
diff -ruN 7zip-Dark7zip/DOC/readme.txt 7zip-Dark7zip2/DOC/readme.txt
--- 7zip-Dark7zip/DOC/readme.txt	2024-01-19 10:53:03.209213101 -0800
+++ 7zip-Dark7zip2/DOC/readme.txt	2024-01-19 10:53:47.679213589 -0800
@@ -155,6 +155,9 @@
   Note that JWasm doesn't support AES instructions. So AES code from C version AesOpt.c 
   will be used instead of assembler code from AesOpt.asm.
 
+USE_AVX512=1
+  use AVX512 instrinsics in AVX512 supported machine.
+
 DISABLE_RAR=1
   removes whole RAR related code from compilation
   
